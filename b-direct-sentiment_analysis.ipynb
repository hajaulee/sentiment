{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read pre-trained dictionary file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import emoji\n",
    "import pandas as pd\n",
    "\n",
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r', encoding=\"utf8\") as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "        \n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "    return words_to_index, index_to_words, word_to_vec_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read input data file (format csv, colume 1: sentence, colume 2: label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(filename):\n",
    "    phrase = []\n",
    "    emoji = []\n",
    "\n",
    "    with open (filename) as csvDataFile:\n",
    "        csvReader = csv.reader(csvDataFile)\n",
    "\n",
    "        for row in csvReader:\n",
    "            phrase.append(row[0])\n",
    "            emoji.append(row[1])\n",
    "\n",
    "    X = np.asarray(phrase)\n",
    "    Y = np.asarray(emoji, dtype=int)\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def read_corpus(filename):\n",
    "    phrase = []\n",
    "    emoji = []\n",
    "\n",
    "    with open (filename, encoding='latin-1') as csvDataFile:\n",
    "        csvReader = csv.reader(csvDataFile)\n",
    "        next(csvReader, None)  # skip the headers\n",
    "        try:    \n",
    "            for row in csvReader:\n",
    "                phrase.append(' '.join(re.findall(r\"[\\w']+\", row[0])))\n",
    "                emoji.append(0 if row[1].strip() == '__label__1' else 1)\n",
    "        except Exception as a:\n",
    "            pass\n",
    "\n",
    "    X = np.asarray(phrase)\n",
    "    Y = np.asarray(emoji, dtype=int)\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_one_hot(Y, C):\n",
    "    Y = np.eye(C)[Y.reshape(-1)]\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode sentence to indices of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    \"\"\"\n",
    "    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.\n",
    "    The output shape should be such that it can be given to `Embedding()`\n",
    "\n",
    "    Arguments:\n",
    "    X -- array of sentences (strings), of shape (m, 1)\n",
    "    word_to_index -- a dictionary containing the each word mapped to its index\n",
    "    max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this.\n",
    "\n",
    "    Returns:\n",
    "    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)\n",
    "    \"\"\"\n",
    "\n",
    "    m = X.shape[0]  # number of training examples\n",
    "    # Initialize X_indices as a numpy matrix of zeros and the correct shape (â‰ˆ 1 line)\n",
    "    X_indices = np.zeros((m, max_len))\n",
    "\n",
    "    for i in range(m):  # loop over training examples\n",
    "        # Convert the ith training sentence in lower case and split is into words. You should get a list of words.\n",
    "        sentence_words = (X[i].lower()).split()\n",
    "        # Initialize j to 0\n",
    "        j = 0\n",
    "        # Loop over the words of sentence_words\n",
    "        for w in sentence_words:\n",
    "            # Set the (i,j)th entry of X_indices to the index of the correct word.\n",
    "            if w in word_to_index:\n",
    "                X_indices[i, j] = word_to_index[w]\n",
    "                # Increment j to j + 1\n",
    "                j = j + 1\n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data, encode input data and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X, Y, C, word_to_index, max_len):\n",
    "    # Convert each sentence to array of index in dictionary\n",
    "    X_indices = sentences_to_indices(X, word_to_index, max_len)\n",
    "\n",
    "    # Converts label to OneHot vector with len = C\n",
    "    Y_oh = convert_to_one_hot(Y, C=C)\n",
    "    return X_indices, Y_oh\n",
    "\n",
    "\n",
    "def label_to_emoji(label):\n",
    "    \"\"\"\n",
    "    Converts a label (int or string) into the corresponding emoji code (string) ready to be printed\n",
    "    \"\"\"\n",
    "    emoji_dictionary = {\"1\": \"\\u2764\\uFE0F\",    # :heart: prints a black instead of red heart depending on the font\n",
    "                        \"0\": \":disappointed:\"}\n",
    "    return emoji.emojize(emoji_dictionary[str(label)], use_aliases=True)\n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=FutureWarning)\n",
    "import numpy as np\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.\n",
    "\n",
    "    Arguments:\n",
    "    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "    Returns:\n",
    "    embedding_layer -- pretrained layer Keras instance\n",
    "    \"\"\"\n",
    "    vocab_len = len(word_to_index) + 1  # adding 1 to fit Keras embedding (requirement)\n",
    "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]  # define dimensionality of your GloVe word vectors (= 50)\n",
    "\n",
    "    # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)\n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "\n",
    "    # Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index, :] = word_to_vec_map[word]\n",
    "\n",
    "    # Define Keras embedding layer with the correct output/input sizes, make it trainable. Use Embedding(...). Make sure to set trainable=False.\n",
    "    embedding_layer = Embedding(vocab_len, emb_dim)\n",
    "\n",
    "    # Build the embedding layer, it is required before setting the weights of the embedding layer. Do not modify the \"None\".\n",
    "    embedding_layer.build((None,))\n",
    "\n",
    "    # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "\n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation, LeakyReLU, Bidirectional, SimpleRNN, GRU\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.callbacks import ModelCheckpoint, Callback\n",
    "import datetime\n",
    "\n",
    "\n",
    "def SentimentAnalysis(input_shape, classes_nb, embedding_layer, summary=False):\n",
    "    # Define sentence_indices as the input of the graph, it should be of shape input_shape and dtype 'int32' (as it contains indices).\n",
    "    sentence_indices = Input(shape=input_shape, dtype=np.int32)\n",
    "\n",
    "    \n",
    "    embeddings = embedding_layer(sentence_indices)\n",
    "    \n",
    "    X = Bidirectional(LSTM(128, return_sequences=True))(embeddings)\n",
    "#     X = SimpleRNN(128, return_sequences=True)(embeddings)\n",
    "#     X = GRU(128, return_sequences=True)(embeddings)\n",
    "    X = Dropout(0.4)(X)\n",
    "    X = Bidirectional(LSTM(128))(X)\n",
    "#     X = SimpleRNN(128)(X)\n",
    "#     X = GRU(128)(X)\n",
    "    \n",
    "    X = Dropout(0.4)(X)\n",
    "    # X = Dense(256)(X)\n",
    "    # X = LeakyReLU(alpha=0.15)(X)\n",
    "    # X = Dropout(0.5)(X)\n",
    "    # X = Dense(128)(X)\n",
    "    X = LeakyReLU(alpha=0.15)(X)\n",
    "    \n",
    "    X = Dense(classes_nb)(X)\n",
    "    # Add a softmax activation\n",
    "    X = Activation('softmax')(X)\n",
    "\n",
    "    # Create Model instance which converts sentence_indices into X.\n",
    "    model = Model(sentence_indices, X)\n",
    "\n",
    "    # Show summary of model\n",
    "    if summary: model.summary()\n",
    "\n",
    "    # Compiple model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "class TrainingHistory(Callback):\n",
    "  def __init__(self, net):\n",
    "    self.net = net\n",
    "  \n",
    "  def on_train_begin(self, logs={}):\n",
    "    self.histories = []\n",
    "\n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "    self.histories.append([logs.get('loss'), logs.get('accuracy'), \n",
    "                           logs.get('val_loss'), logs.get('val_accuracy')])\n",
    "    np.savetxt(\"./model/\"+self.net+\"-history.txt\", self.histories, delimiter=\",\")\n",
    "\n",
    "\n",
    "def today():\n",
    "  return '{:02d}{:02d}'.format(datetime.date.today().month, datetime.date.today().day)\n",
    "\n",
    "def callbacks(net_name):\n",
    "    # checkpoint\n",
    "    filepath=\"./model/\"+net_name+\"-\"+ today() +\"-weights-{epoch:02d}-{loss:.2f}-{accuracy:.2f}-{val_loss:.2f}-{val_accuracy:.2f}.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "    log_history = TrainingHistory(net_name)\n",
    "#     callbacks = [checkpoint, log_history]\n",
    "    _callbacks = [log_history]\n",
    "    return _callbacks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=FutureWarning)\n",
    "import sys\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from data_sources import *    \n",
    "from visualization import *\n",
    "from embedding import *\n",
    "from model import *\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dictionary and input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw data\n",
      "Loading dictionary file.\n"
     ]
    }
   ],
   "source": [
    "# Read train and test files\n",
    "print(\"Loading raw data\")\n",
    "# X, Y = read_csv('imdb_labelled.txt.csv')\n",
    "X, Y = read_corpus('corpus.csv')\n",
    "# Read 50 feature dimension glove file\n",
    "print(\"Loading dictionary file.\")\n",
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess data and split train/test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES_NUMBER = 2\n",
    "# Compute max length of sentences set\n",
    "maxLen = len(max(X, key= lambda x : len(x.split())).split())\n",
    "# Preprocess input data\n",
    "X, Y = preprocess_data(X, Y, CLASSES_NUMBER, word_to_index, maxLen)\n",
    "\n",
    "# Split train/test set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "def softmax_stable(Z):\n",
    "    \"\"\"\n",
    "    Compute softmax values for each sets of scores in Z.\n",
    "    each column of Z is a set of score.    \n",
    "    \"\"\"\n",
    "    e_Z = np.exp(Z - np.max(Z, axis = 0, keepdims = True))\n",
    "    A = e_Z / e_Z.sum(axis = 0)\n",
    "    return A\n",
    "\n",
    "softmax_stable(np.asarray([0.832323, 1.532323]))\n",
    "print(np.argmax([0.33181223, 0.66818777]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[185457. 226278. 358160.  80586.      0.      0.      0.      0.      0.\n",
      "       0.]]\n",
      "[[[ 1.1891e-01  1.5255e-01 -8.2073e-02 -7.4144e-01  7.5917e-01\n",
      "   -4.8328e-01 -3.1009e-01  5.1476e-01 -9.8708e-01  6.1757e-04\n",
      "   -1.5043e-01  8.3770e-01 -1.0797e+00 -5.1460e-01  1.3188e+00\n",
      "    6.2007e-01  1.3779e-01  4.7108e-01 -7.2874e-02 -7.2675e-01\n",
      "   -7.4116e-01  7.5263e-01  8.8180e-01  2.9561e-01  1.3548e+00\n",
      "   -2.5701e+00 -1.3523e+00  4.5880e-01  1.0068e+00 -1.1856e+00\n",
      "    3.4737e+00  7.7898e-01 -7.2929e-01  2.5102e-01 -2.6156e-01\n",
      "   -3.4684e-01  5.5841e-01  7.5098e-01  4.9830e-01 -2.6823e-01\n",
      "   -2.7443e-03 -1.8298e-02 -2.8096e-01  5.5318e-01  3.7706e-02\n",
      "    1.8555e-01 -1.5025e-01 -5.7512e-01 -2.6671e-01  9.2121e-01]\n",
      "  [-1.3886e-01  1.1401e+00 -8.5212e-01 -2.9212e-01  7.5534e-01\n",
      "    8.2762e-01 -3.1810e-01  7.2204e-03 -3.4762e-01  1.0731e+00\n",
      "   -2.4665e-01  9.7765e-01 -5.5835e-01 -9.0318e-02  8.3182e-01\n",
      "   -3.3317e-01  2.2648e-01  3.0913e-01  2.6929e-02 -8.6739e-02\n",
      "   -1.4703e-01  1.3543e+00  5.3695e-01  4.3735e-01  1.2749e+00\n",
      "   -1.4382e+00 -1.2815e+00 -1.5196e-01  1.0506e+00 -9.3644e-01\n",
      "    2.7561e+00  5.8967e-01 -2.9473e-01  2.7574e-01 -3.2928e-01\n",
      "   -2.0100e-01 -2.8547e-01 -4.5987e-01 -1.4603e-01 -6.9372e-01\n",
      "    7.0761e-02 -1.9326e-01 -1.8550e-01 -1.6095e-01  2.4268e-01\n",
      "    2.0784e-01  3.0924e-02 -1.3711e+00 -2.8606e-01  2.8980e-01]\n",
      "  [ 5.3074e-01  4.0117e-01 -4.0785e-01  1.5444e-01  4.7782e-01\n",
      "    2.0754e-01 -2.6951e-01 -3.4023e-01 -1.0879e-01  1.0563e-01\n",
      "   -1.0289e-01  1.0849e-01 -4.9681e-01 -2.5128e-01  8.4025e-01\n",
      "    3.8949e-01  3.2284e-01 -2.2797e-01 -4.4342e-01 -3.1649e-01\n",
      "   -1.2406e-01 -2.8170e-01  1.9467e-01  5.5513e-02  5.6705e-01\n",
      "   -1.7419e+00 -9.1145e-01  2.7036e-01  4.1927e-01  2.0279e-02\n",
      "    4.0405e+00 -2.4943e-01 -2.0416e-01 -6.2762e-01 -5.4783e-02\n",
      "   -2.6883e-01  1.8444e-01  1.8204e-01 -2.3536e-01 -1.6155e-01\n",
      "   -2.7655e-01  3.5506e-02 -3.8211e-01 -7.5134e-04 -2.4822e-01\n",
      "    2.8164e-01  1.2819e-01  2.8762e-01  1.4440e-01  2.3611e-01]\n",
      "  [-7.6543e-03  9.3456e-01 -7.3189e-01 -5.5162e-01  7.6977e-01\n",
      "    3.5925e-01 -1.1365e+00 -1.1632e+00  3.4214e-01  2.9145e-01\n",
      "   -8.7110e-01  9.1970e-01 -4.7069e-01 -2.2834e-01  1.4777e+00\n",
      "   -8.1714e-01 -1.7466e-01 -5.1093e-01 -2.8354e-01  2.3292e-01\n",
      "    7.1832e-01  2.3414e-01  4.9443e-01  3.5483e-01  7.6889e-01\n",
      "   -1.4374e+00 -1.7457e+00 -2.8994e-01 -1.0156e-01 -3.6959e-01\n",
      "    2.5502e+00 -1.0581e+00 -4.9416e-02 -2.5524e-01 -6.3303e-01\n",
      "    2.6710e-02 -1.8733e-01  2.0206e-01 -2.6288e-01 -4.1418e-01\n",
      "    8.3473e-01 -1.4227e-01 -2.8125e-01  9.8155e-02 -1.7096e-01\n",
      "    5.2408e-01  3.1851e-01 -8.9847e-02 -2.7223e-01 -8.8736e-03]\n",
      "  [ 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "    0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "    0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "    0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "    0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "    0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "    0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "    0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "    0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "    0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00]\n",
      "  [ 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "    0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "    0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "    0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "    0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "    0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "    0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "    0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "    0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "    0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00]\n",
      "  [ 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "    0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "    0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "    0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "    0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "    0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "    0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "    0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "    0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "    0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00]\n",
      "  [ 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "    0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "    0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "    0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "    0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "    0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "    0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "    0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "    0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "    0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00]\n",
      "  [ 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "    0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "    0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "    0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "    0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "    0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "    0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "    0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "    0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "    0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00]\n",
      "  [ 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "    0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "    0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "    0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "    0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "    0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "    0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "    0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "    0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "    0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00]]]\n"
     ]
    }
   ],
   "source": [
    "def Embedder(input_shape, embedding_layer, summary=False):\n",
    "    # Define sentence_indices as the input of the graph, it should be of shape input_shape and dtype 'int32' (as it contains indices).\n",
    "    sentence_indices = Input(shape=input_shape, dtype=np.int32)\n",
    "    embeddings = embedding_layer(sentence_indices)\n",
    "    model = Model(sentence_indices, embeddings)\n",
    "    # Show summary of model\n",
    "    if summary: model.summary()\n",
    "    # Compiple model\n",
    "#     model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "raw_s = \"I love this book\"\n",
    "input_s = sentences_to_indices(np.asarray([raw_s]), word_to_index, 10)\n",
    "print(input_s)\n",
    "embedder = Embedder((10,), embedding_layer)\n",
    "embedded_s = embedder.predict([input_s])\n",
    "print(embedded_s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/hajau/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 215)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 215, 50)           20000050  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 215, 128)          91648     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 215, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 20,223,540\n",
      "Trainable params: 223,490\n",
      "Non-trainable params: 20,000,050\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the embedding layer pretrained with GloVe Vectors (â‰ˆ1 line)\n",
    "embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "\n",
    "embedding_layer.trainable=False\n",
    "# Model and model summmary\n",
    "model = SentimentAnalysis((maxLen,), CLASSES_NUMBER, embedding_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The number of classes has to be greater than one; got 1 class",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-b69ed2ba46ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    145\u001b[0m                          \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'C'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                          accept_large_sparse=False)\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         sample_weight = np.asarray([]\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_validate_targets\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    519\u001b[0m             raise ValueError(\n\u001b[1;32m    520\u001b[0m                 \u001b[0;34m\"The number of classes has to be greater than one; got %d\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m                 \" class\" % len(cls))\n\u001b[0m\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The number of classes has to be greater than one; got 1 class"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC(C=10**4, gamma='auto',verbose=True, max_iter=100,random_state=2)\n",
    "clf.fit(X_train, 1+np.argmax(Y_train, axis=1))\n",
    "sum(clf.predict(X_test) == (1+np.argmax(Y_test, axis=1))) / len(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train model in iter 0\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      "8000/8000 [==============================] - 154s 19ms/step - loss: 0.6056 - accuracy: 0.6660 - val_loss: 0.5246 - val_accuracy: 0.7430\n",
      "Epoch 2/50\n",
      "8000/8000 [==============================] - 148s 19ms/step - loss: 0.5041 - accuracy: 0.7638 - val_loss: 0.4822 - val_accuracy: 0.7750\n",
      "Epoch 3/50\n",
      "8000/8000 [==============================] - 147s 18ms/step - loss: 0.4582 - accuracy: 0.7901 - val_loss: 0.5048 - val_accuracy: 0.7670\n",
      "Epoch 4/50\n",
      "8000/8000 [==============================] - 151s 19ms/step - loss: 0.4465 - accuracy: 0.7993 - val_loss: 0.4579 - val_accuracy: 0.7805\n",
      "Epoch 5/50\n",
      "8000/8000 [==============================] - 150s 19ms/step - loss: 0.4256 - accuracy: 0.8120 - val_loss: 0.4488 - val_accuracy: 0.7915\n",
      "Epoch 6/50\n",
      "8000/8000 [==============================] - 181s 23ms/step - loss: 0.4153 - accuracy: 0.8083 - val_loss: 0.4181 - val_accuracy: 0.8065\n",
      "Epoch 7/50\n",
      "8000/8000 [==============================] - 156s 19ms/step - loss: 0.3932 - accuracy: 0.8271 - val_loss: 0.4422 - val_accuracy: 0.8030\n",
      "Epoch 8/50\n",
      "8000/8000 [==============================] - 154s 19ms/step - loss: 0.3848 - accuracy: 0.8298 - val_loss: 0.4044 - val_accuracy: 0.8160\n",
      "Epoch 9/50\n",
      "8000/8000 [==============================] - 149s 19ms/step - loss: 0.3700 - accuracy: 0.8375 - val_loss: 0.3929 - val_accuracy: 0.8215\n",
      "Epoch 10/50\n",
      "8000/8000 [==============================] - 146s 18ms/step - loss: 0.3562 - accuracy: 0.8499 - val_loss: 0.3974 - val_accuracy: 0.8175\n",
      "Epoch 11/50\n",
      "8000/8000 [==============================] - 154s 19ms/step - loss: 0.3670 - accuracy: 0.8376 - val_loss: 0.3796 - val_accuracy: 0.8290\n",
      "Epoch 12/50\n",
      "8000/8000 [==============================] - 156s 20ms/step - loss: 0.3359 - accuracy: 0.8554 - val_loss: 0.3828 - val_accuracy: 0.8320\n",
      "Epoch 13/50\n",
      "8000/8000 [==============================] - 157s 20ms/step - loss: 0.3174 - accuracy: 0.8664 - val_loss: 0.3866 - val_accuracy: 0.8395\n",
      "Epoch 14/50\n",
      "8000/8000 [==============================] - 154s 19ms/step - loss: 0.3284 - accuracy: 0.8618 - val_loss: 0.3787 - val_accuracy: 0.8430\n",
      "Epoch 15/50\n",
      "8000/8000 [==============================] - 156s 19ms/step - loss: 0.2870 - accuracy: 0.8809 - val_loss: 0.3537 - val_accuracy: 0.8480\n",
      "Epoch 16/50\n",
      "8000/8000 [==============================] - 156s 20ms/step - loss: 0.2811 - accuracy: 0.8846 - val_loss: 0.3725 - val_accuracy: 0.8370\n",
      "Epoch 17/50\n",
      "8000/8000 [==============================] - 155s 19ms/step - loss: 0.2694 - accuracy: 0.8875 - val_loss: 0.3678 - val_accuracy: 0.8530\n",
      "Epoch 18/50\n",
      "8000/8000 [==============================] - 156s 20ms/step - loss: 0.2514 - accuracy: 0.8972 - val_loss: 0.4219 - val_accuracy: 0.8195\n",
      "Epoch 19/50\n",
      "8000/8000 [==============================] - 155s 19ms/step - loss: 0.2626 - accuracy: 0.8914 - val_loss: 0.3582 - val_accuracy: 0.8540\n",
      "Epoch 20/50\n",
      "8000/8000 [==============================] - 156s 20ms/step - loss: 0.2416 - accuracy: 0.9034 - val_loss: 0.3969 - val_accuracy: 0.8430\n",
      "Epoch 21/50\n",
      "8000/8000 [==============================] - 156s 19ms/step - loss: 0.2279 - accuracy: 0.9082 - val_loss: 0.4798 - val_accuracy: 0.8355\n",
      "Epoch 22/50\n",
      "8000/8000 [==============================] - 156s 19ms/step - loss: 0.2339 - accuracy: 0.9061 - val_loss: 0.4162 - val_accuracy: 0.8315\n",
      "Epoch 23/50\n",
      "8000/8000 [==============================] - 156s 20ms/step - loss: 0.2042 - accuracy: 0.9196 - val_loss: 0.4047 - val_accuracy: 0.8370\n",
      "Epoch 24/50\n",
      "8000/8000 [==============================] - 157s 20ms/step - loss: 0.1851 - accuracy: 0.9286 - val_loss: 0.4584 - val_accuracy: 0.8390\n",
      "Epoch 25/50\n",
      "8000/8000 [==============================] - 154s 19ms/step - loss: 0.1532 - accuracy: 0.9430 - val_loss: 0.5127 - val_accuracy: 0.8440\n",
      "Epoch 26/50\n",
      "8000/8000 [==============================] - 157s 20ms/step - loss: 0.1688 - accuracy: 0.9364 - val_loss: 0.4460 - val_accuracy: 0.8490\n",
      "Epoch 27/50\n",
      "8000/8000 [==============================] - 156s 19ms/step - loss: 0.1508 - accuracy: 0.9435 - val_loss: 0.4785 - val_accuracy: 0.8450\n",
      "Epoch 28/50\n",
      "8000/8000 [==============================] - 156s 20ms/step - loss: 0.1863 - accuracy: 0.9266 - val_loss: 0.4057 - val_accuracy: 0.8175\n",
      "Epoch 29/50\n",
      "8000/8000 [==============================] - 156s 20ms/step - loss: 0.2366 - accuracy: 0.9064 - val_loss: 0.4672 - val_accuracy: 0.8395\n",
      "Epoch 30/50\n",
      "8000/8000 [==============================] - 155s 19ms/step - loss: 0.1688 - accuracy: 0.9373 - val_loss: 0.4633 - val_accuracy: 0.8520\n",
      "Epoch 31/50\n",
      "8000/8000 [==============================] - 156s 19ms/step - loss: 0.1350 - accuracy: 0.9523 - val_loss: 0.5026 - val_accuracy: 0.8475\n",
      "Epoch 32/50\n",
      "8000/8000 [==============================] - 155s 19ms/step - loss: 0.1201 - accuracy: 0.9541 - val_loss: 0.5725 - val_accuracy: 0.8390\n",
      "Epoch 33/50\n",
      "6144/8000 [======================>.......] - ETA: 33s - loss: 0.0946 - accuracy: 0.9660"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "NET_NAME = 'bi-direct'\n",
    "for try_count in range(30):\n",
    "    print(\"Train model in iter {}\".format(try_count))\n",
    "    model = SentimentAnalysis((maxLen,), CLASSES_NUMBER, embedding_layer, summary=False)\n",
    "    net_name = NET_NAME + str(try_count) + '-' + str(epochs)\n",
    "    model.fit(X_train, Y_train, \n",
    "              epochs=epochs, \n",
    "              batch_size=256, \n",
    "              workers=8,\n",
    "              verbose=1,\n",
    "              shuffle=True,\n",
    "              use_multiprocessing=True,\n",
    "              validation_data=(X_test, Y_test), \n",
    "              callbacks=callbacks(net_name))\n",
    "\n",
    "    # Visualize History of Traing model\n",
    "    show_training_history(\"./model/\"+net_name+\"-history.txt\")\n",
    "    # Evaluate model, loss and accuracy\n",
    "    loss, acc = model.evaluate(X_test, Y_test, verbose=1)\n",
    "    print()\n",
    "    print(\"Test accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from model/sentiment-1107-weights-31-0.10-0.98-0.69-0.84.hdf5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You are trying to load a weight file containing 6 layers into a model with 4 layers.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-a5af1f7ee0ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'model/sentiment-1107-weights-31-0.10-0.98-0.69-0.84.hdf5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading weights from\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Evaluate model, loss and accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mload_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, reshape)\u001b[0m\n\u001b[1;32m   1228\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1229\u001b[0m                 saving.load_weights_from_hdf5_group(\n\u001b[0;32m-> 1230\u001b[0;31m                     f, self.layers, reshape=reshape)\n\u001b[0m\u001b[1;32m   1231\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'close'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers, reshape)\u001b[0m\n\u001b[1;32m   1207\u001b[0m                          \u001b[0;34m'containing '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m                          \u001b[0;34m' layers into a model with '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1209\u001b[0;31m                          str(len(filtered_layers)) + ' layers.')\n\u001b[0m\u001b[1;32m   1210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1211\u001b[0m     \u001b[0;31m# We batch weight value assignments in a single backend call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: You are trying to load a weight file containing 6 layers into a model with 4 layers."
     ]
    }
   ],
   "source": [
    "model_path = 'model/sentiment-1107-weights-31-0.10-0.98-0.69-0.84.hdf5'\n",
    "print(\"Loading weights from\", model_path)\n",
    "model.load_weights(model_path)\n",
    "\n",
    "# Evaluate model, loss and accuracy\n",
    "loss, acc = model.evaluate(X, Y, verbose=1)\n",
    "print()\n",
    "print(\"Test accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "./model/sentiment-history.txt not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-b358b054c8e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mshow_training_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./model/sentiment-history.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-b358b054c8e6>\u001b[0m in \u001b[0;36mshow_training_history\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mshow_training_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mhistories\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistories\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistories\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows)\u001b[0m\n\u001b[1;32m    966\u001b[0m             \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_string_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m             \u001b[0mfencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'encoding'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[1;32m    621\u001b[0m                                       encoding=encoding, newline=newline)\n\u001b[1;32m    622\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s not found.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: ./model/sentiment-history.txt not found."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt \n",
    "\n",
    "def show_training_history(file_path):\n",
    "    histories=np.loadtxt(file_path, delimiter=\",\")\n",
    "    plt.plot(histories[:, 1])\n",
    "    plt.plot(histories[:, 3])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    # summarize history for loss\n",
    "    plt.plot(histories[:, 0])\n",
    "    plt.plot(histories[:, 2])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "show_training_history(\"./model/sentiment-history.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
