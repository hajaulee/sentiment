{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read pre-trained dictionary file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import emoji\n",
    "import pandas as pd\n",
    "\n",
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r', encoding=\"utf8\") as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "        \n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "    return words_to_index, index_to_words, word_to_vec_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read input data file (format csv, colume 1: sentence, colume 2: label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(filename):\n",
    "    phrase = []\n",
    "    emoji = []\n",
    "\n",
    "    with open (filename) as csvDataFile:\n",
    "        csvReader = csv.reader(csvDataFile)\n",
    "\n",
    "        for row in csvReader:\n",
    "            phrase.append(row[0])\n",
    "            emoji.append(row[1])\n",
    "\n",
    "    X = np.asarray(phrase)\n",
    "    Y = np.asarray(emoji, dtype=int)\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_one_hot(Y, C):\n",
    "    Y = np.eye(C)[Y.reshape(-1)]\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode sentence to indices of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    \"\"\"\n",
    "    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.\n",
    "    The output shape should be such that it can be given to `Embedding()`\n",
    "\n",
    "    Arguments:\n",
    "    X -- array of sentences (strings), of shape (m, 1)\n",
    "    word_to_index -- a dictionary containing the each word mapped to its index\n",
    "    max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this.\n",
    "\n",
    "    Returns:\n",
    "    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)\n",
    "    \"\"\"\n",
    "\n",
    "    m = X.shape[0]  # number of training examples\n",
    "    # Initialize X_indices as a numpy matrix of zeros and the correct shape (â‰ˆ 1 line)\n",
    "    X_indices = np.zeros((m, max_len))\n",
    "\n",
    "    for i in range(m):  # loop over training examples\n",
    "        # Convert the ith training sentence in lower case and split is into words. You should get a list of words.\n",
    "        sentence_words = (X[i].lower()).split()\n",
    "        # Initialize j to 0\n",
    "        j = 0\n",
    "        # Loop over the words of sentence_words\n",
    "        for w in sentence_words:\n",
    "            # Set the (i,j)th entry of X_indices to the index of the correct word.\n",
    "            if w in word_to_index:\n",
    "                X_indices[i, j] = word_to_index[w]\n",
    "                # Increment j to j + 1\n",
    "                j = j + 1\n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data, encode input data and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X, Y, C, word_to_index, max_len):\n",
    "    # Convert each sentence to array of index in dictionary\n",
    "    X_indices = sentences_to_indices(X, word_to_index, max_len)\n",
    "\n",
    "    # Converts label to OneHot vector with len = C\n",
    "    Y_oh = convert_to_one_hot(Y, C=C)\n",
    "    return X_indices, Y_oh\n",
    "\n",
    "\n",
    "def label_to_emoji(label):\n",
    "    \"\"\"\n",
    "    Converts a label (int or string) into the corresponding emoji code (string) ready to be printed\n",
    "    \"\"\"\n",
    "    emoji_dictionary = {\"1\": \"\\u2764\\uFE0F\",    # :heart: prints a black instead of red heart depending on the font\n",
    "                        \"0\": \":disappointed:\"}\n",
    "    return emoji.emojize(emoji_dictionary[str(label)], use_aliases=True)\n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=FutureWarning)\n",
    "import numpy as np\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.\n",
    "\n",
    "    Arguments:\n",
    "    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "    Returns:\n",
    "    embedding_layer -- pretrained layer Keras instance\n",
    "    \"\"\"\n",
    "    vocab_len = len(word_to_index) + 1  # adding 1 to fit Keras embedding (requirement)\n",
    "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]  # define dimensionality of your GloVe word vectors (= 50)\n",
    "\n",
    "    # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)\n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "\n",
    "    # Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index, :] = word_to_vec_map[word]\n",
    "\n",
    "    # Define Keras embedding layer with the correct output/input sizes, make it trainable. Use Embedding(...). Make sure to set trainable=False.\n",
    "    embedding_layer = Embedding(vocab_len, emb_dim)\n",
    "\n",
    "    # Build the embedding layer, it is required before setting the weights of the embedding layer. Do not modify the \"None\".\n",
    "    embedding_layer.build((None,))\n",
    "\n",
    "    # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "\n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation, LeakyReLU\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.callbacks import ModelCheckpoint, Callback\n",
    "import datetime\n",
    "\n",
    "\n",
    "def SentimentAnalysis(input_shape, classes_nb, embedding_layer):\n",
    "    # Define sentence_indices as the input of the graph, it should be of shape input_shape and dtype 'int32' (as it contains indices).\n",
    "    sentence_indices = Input(shape=input_shape, dtype=np.int32)\n",
    "\n",
    "    # Propagate sentence_indices through your embedding layer, you get back the embeddings\n",
    "    embeddings = embedding_layer(sentence_indices)\n",
    "\n",
    "    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n",
    "    # Be careful, the returned output should be a batch of sequences.\n",
    "    X = LSTM(128, return_sequences=True)(embeddings)\n",
    "    # Add dropout with a probability of 0.5\n",
    "    X = Dropout(0.5)(X)\n",
    "    # Propagate X trough another LSTM layer with 128-dimensional hidden state\n",
    "    # Be careful, the returned output should be a single hidden state, not a batch of sequences.\n",
    "    X = LSTM(128)(X)\n",
    "    # Add dropout with a probability of 0.5\n",
    "    X = Dropout(0.5)(X)\n",
    "    # X = Dense(256)(X)\n",
    "    # X = LeakyReLU(alpha=0.15)(X)\n",
    "    # X = Dropout(0.5)(X)\n",
    "    # X = Dense(128)(X)\n",
    "    X = LeakyReLU(alpha=0.15)(X)\n",
    "    \n",
    "    X = Dense(classes_nb)(X)\n",
    "    # Add a softmax activation\n",
    "    X = Activation('softmax')(X)\n",
    "\n",
    "    # Create Model instance which converts sentence_indices into X.\n",
    "    model = Model(sentence_indices, X)\n",
    "\n",
    "    # Show summary of model\n",
    "    model.summary()\n",
    "\n",
    "    # Compiple model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "class TrainingHistory(Callback):\n",
    "  def __init__(self, net):\n",
    "    self.net = net\n",
    "  \n",
    "  def on_train_begin(self, logs={}):\n",
    "    self.histories = []\n",
    "\n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "    self.histories.append([logs.get('loss'), logs.get('accuracy'), \n",
    "                           logs.get('val_loss'), logs.get('val_accuracy')])\n",
    "    np.savetxt(\"./model/\"+self.net+\"-history.txt\", self.histories, delimiter=\",\")\n",
    "\n",
    "\n",
    "def today():\n",
    "  return '{:02d}{:02d}'.format(datetime.date.today().month, datetime.date.today().day)\n",
    "\n",
    "def callbacks(net_name):\n",
    "    # checkpoint\n",
    "    filepath=\"./model/\"+net_name+\"-\"+ today() +\"-weights-{epoch:02d}-{loss:.2f}-{accuracy:.2f}-{val_loss:.2f}-{val_accuracy:.2f}.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "    log_history = TrainingHistory(net_name)\n",
    "    callbacks = [checkpoint, log_history]\n",
    "    return callbacks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=FutureWarning)\n",
    "import sys\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from data_sources import *    \n",
    "from visualization import *\n",
    "from embedding import *\n",
    "from model import *\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dictionary and input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw data\n",
      "Loading dictionary file.\n"
     ]
    }
   ],
   "source": [
    "# Read train and test files\n",
    "print(\"Loading raw data\")\n",
    "X, Y = read_csv('imdb_labelled.txt.csv')\n",
    "\n",
    "# Read 50 feature dimension glove file\n",
    "print(\"Loading dictionary file.\")\n",
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess data and split train/test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES_NUMBER = 2\n",
    "NET_NAME = 'sentiment'\n",
    "# Compute max length of sentences set\n",
    "maxLen = len(max(X, key=len).split())\n",
    "\n",
    "# Preprocess input data\n",
    "X, Y = preprocess_data(X, Y, CLASSES_NUMBER, word_to_index, maxLen)\n",
    "\n",
    "# Split train/test set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 74)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 74, 50)            20000050  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 74, 128)           91648     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 74, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 20,223,540\n",
      "Trainable params: 20,223,540\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the embedding layer pretrained with GloVe Vectors (â‰ˆ1 line)\n",
    "embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "# Model and model summmary\n",
    "model = SentimentAnalysis((maxLen,), CLASSES_NUMBER, embedding_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6927 - accuracy: 0.5125 - val_loss: 0.6933 - val_accuracy: 0.4700\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.47000, saving model to ./model/sentiment2000-1124-weights-01-0.69-0.51-0.69-0.47.hdf5\n",
      "Epoch 2/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6630 - accuracy: 0.6313 - val_loss: 0.9461 - val_accuracy: 0.6100\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.47000 to 0.61000, saving model to ./model/sentiment2000-1124-weights-02-0.66-0.63-0.95-0.61.hdf5\n",
      "Epoch 3/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6917 - accuracy: 0.6212 - val_loss: 0.6532 - val_accuracy: 0.6200\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.61000 to 0.62000, saving model to ./model/sentiment2000-1124-weights-03-0.69-0.62-0.65-0.62.hdf5\n",
      "Epoch 4/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6296 - accuracy: 0.6363 - val_loss: 0.6425 - val_accuracy: 0.6600\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.62000 to 0.66000, saving model to ./model/sentiment2000-1124-weights-04-0.63-0.64-0.64-0.66.hdf5\n",
      "Epoch 5/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.5640 - accuracy: 0.7513 - val_loss: 0.6125 - val_accuracy: 0.6900\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.66000 to 0.69000, saving model to ./model/sentiment2000-1124-weights-05-0.56-0.75-0.61-0.69.hdf5\n",
      "Epoch 6/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.4764 - accuracy: 0.7763 - val_loss: 0.5642 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.69000 to 0.77000, saving model to ./model/sentiment2000-1124-weights-06-0.48-0.78-0.56-0.77.hdf5\n",
      "Epoch 7/2000\n",
      "800/800 [==============================] - 7s 8ms/step - loss: 0.4500 - accuracy: 0.8188 - val_loss: 0.5302 - val_accuracy: 0.7450\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.77000\n",
      "Epoch 8/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.4144 - accuracy: 0.8400 - val_loss: 0.5269 - val_accuracy: 0.7550\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.77000\n",
      "Epoch 9/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.3143 - accuracy: 0.8888 - val_loss: 0.5638 - val_accuracy: 0.7400\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.77000\n",
      "Epoch 10/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.2972 - accuracy: 0.8938 - val_loss: 0.7197 - val_accuracy: 0.7550\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.77000\n",
      "Epoch 11/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.3381 - accuracy: 0.8900 - val_loss: 0.5612 - val_accuracy: 0.7800\n",
      "\n",
      "Epoch 00011: val_accuracy improved from 0.77000 to 0.78000, saving model to ./model/sentiment2000-1124-weights-11-0.34-0.89-0.56-0.78.hdf5\n",
      "Epoch 12/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.2856 - accuracy: 0.9187 - val_loss: 0.6256 - val_accuracy: 0.7750\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.78000\n",
      "Epoch 13/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.5082 - accuracy: 0.7825 - val_loss: 0.7045 - val_accuracy: 0.6750\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.78000\n",
      "Epoch 14/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.3930 - accuracy: 0.8900 - val_loss: 0.5637 - val_accuracy: 0.7500\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.78000\n",
      "Epoch 15/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.2932 - accuracy: 0.9025 - val_loss: 0.7875 - val_accuracy: 0.7450\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.78000\n",
      "Epoch 16/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.2616 - accuracy: 0.9200 - val_loss: 0.5497 - val_accuracy: 0.7450\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.78000\n",
      "Epoch 17/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.1938 - accuracy: 0.9463 - val_loss: 0.6041 - val_accuracy: 0.7950\n",
      "\n",
      "Epoch 00017: val_accuracy improved from 0.78000 to 0.79500, saving model to ./model/sentiment2000-1124-weights-17-0.19-0.95-0.60-0.80.hdf5\n",
      "Epoch 18/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.1729 - accuracy: 0.9550 - val_loss: 0.6758 - val_accuracy: 0.8000\n",
      "\n",
      "Epoch 00018: val_accuracy improved from 0.79500 to 0.80000, saving model to ./model/sentiment2000-1124-weights-18-0.17-0.95-0.68-0.80.hdf5\n",
      "Epoch 19/2000\n",
      "800/800 [==============================] - 8s 9ms/step - loss: 0.1519 - accuracy: 0.9588 - val_loss: 0.7099 - val_accuracy: 0.8000\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.80000\n",
      "Epoch 20/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.1504 - accuracy: 0.9650 - val_loss: 0.6523 - val_accuracy: 0.8050\n",
      "\n",
      "Epoch 00020: val_accuracy improved from 0.80000 to 0.80500, saving model to ./model/sentiment2000-1124-weights-20-0.15-0.96-0.65-0.81.hdf5\n",
      "Epoch 21/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.1345 - accuracy: 0.9675 - val_loss: 0.6571 - val_accuracy: 0.7800\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.80500\n",
      "Epoch 22/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.1210 - accuracy: 0.9688 - val_loss: 0.6830 - val_accuracy: 0.8050\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.80500\n",
      "Epoch 23/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.1116 - accuracy: 0.9750 - val_loss: 0.7380 - val_accuracy: 0.7950\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.80500\n",
      "Epoch 24/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.1035 - accuracy: 0.9775 - val_loss: 0.6828 - val_accuracy: 0.7950\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.80500\n",
      "Epoch 25/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.0867 - accuracy: 0.9812 - val_loss: 0.9891 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.80500\n",
      "Epoch 26/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0784 - accuracy: 0.9812 - val_loss: 0.9144 - val_accuracy: 0.8050\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.80500\n",
      "Epoch 27/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.1008 - accuracy: 0.9750 - val_loss: 0.6695 - val_accuracy: 0.8050\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.80500\n",
      "Epoch 28/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0636 - accuracy: 0.9825 - val_loss: 0.7900 - val_accuracy: 0.8100\n",
      "\n",
      "Epoch 00028: val_accuracy improved from 0.80500 to 0.81000, saving model to ./model/sentiment2000-1124-weights-28-0.06-0.98-0.79-0.81.hdf5\n",
      "Epoch 29/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0633 - accuracy: 0.9850 - val_loss: 0.8519 - val_accuracy: 0.8050\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.81000\n",
      "Epoch 30/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0517 - accuracy: 0.9900 - val_loss: 0.9686 - val_accuracy: 0.7950\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.81000\n",
      "Epoch 31/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0968 - accuracy: 0.9787 - val_loss: 0.8457 - val_accuracy: 0.8200\n",
      "\n",
      "Epoch 00031: val_accuracy improved from 0.81000 to 0.82000, saving model to ./model/sentiment2000-1124-weights-31-0.10-0.98-0.85-0.82.hdf5\n",
      "Epoch 32/2000\n",
      "800/800 [==============================] - 8s 9ms/step - loss: 0.0726 - accuracy: 0.9837 - val_loss: 0.6378 - val_accuracy: 0.8300\n",
      "\n",
      "Epoch 00032: val_accuracy improved from 0.82000 to 0.83000, saving model to ./model/sentiment2000-1124-weights-32-0.07-0.98-0.64-0.83.hdf5\n",
      "Epoch 33/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0530 - accuracy: 0.9875 - val_loss: 0.7206 - val_accuracy: 0.8200\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.83000\n",
      "Epoch 34/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0441 - accuracy: 0.9925 - val_loss: 0.7525 - val_accuracy: 0.8300\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.83000\n",
      "Epoch 35/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0446 - accuracy: 0.9925 - val_loss: 0.7850 - val_accuracy: 0.8200\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.83000\n",
      "Epoch 36/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0412 - accuracy: 0.9925 - val_loss: 0.8222 - val_accuracy: 0.8000\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.83000\n",
      "Epoch 37/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0444 - accuracy: 0.9912 - val_loss: 0.8267 - val_accuracy: 0.8100\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.83000\n",
      "Epoch 38/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0417 - accuracy: 0.9912 - val_loss: 0.8078 - val_accuracy: 0.8000\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.83000\n",
      "Epoch 39/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0589 - accuracy: 0.9837 - val_loss: 0.8015 - val_accuracy: 0.8150\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.83000\n",
      "Epoch 40/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0445 - accuracy: 0.9912 - val_loss: 1.0483 - val_accuracy: 0.7850\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.83000\n",
      "Epoch 41/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0396 - accuracy: 0.9937 - val_loss: 0.8298 - val_accuracy: 0.8150\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.83000\n",
      "Epoch 42/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0378 - accuracy: 0.9937 - val_loss: 0.8968 - val_accuracy: 0.8100\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.83000\n",
      "Epoch 43/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0355 - accuracy: 0.9925 - val_loss: 0.9298 - val_accuracy: 0.8200\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 0.83000\n",
      "Epoch 44/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0329 - accuracy: 0.9950 - val_loss: 0.9356 - val_accuracy: 0.8200\n",
      "\n",
      "Epoch 00044: val_accuracy did not improve from 0.83000\n",
      "Epoch 45/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0383 - accuracy: 0.9937 - val_loss: 0.9504 - val_accuracy: 0.8000\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 0.83000\n",
      "Epoch 46/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0411 - accuracy: 0.9912 - val_loss: 0.9345 - val_accuracy: 0.8150\n",
      "\n",
      "Epoch 00046: val_accuracy did not improve from 0.83000\n",
      "Epoch 47/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0315 - accuracy: 0.9937 - val_loss: 0.9726 - val_accuracy: 0.8200\n",
      "\n",
      "Epoch 00047: val_accuracy did not improve from 0.83000\n",
      "Epoch 48/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0505 - accuracy: 0.9900 - val_loss: 1.6230 - val_accuracy: 0.7050\n",
      "\n",
      "Epoch 00048: val_accuracy did not improve from 0.83000\n",
      "Epoch 49/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.5439 - accuracy: 0.8525 - val_loss: 0.6463 - val_accuracy: 0.7750\n",
      "\n",
      "Epoch 00049: val_accuracy did not improve from 0.83000\n",
      "Epoch 50/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.2391 - accuracy: 0.9388 - val_loss: 0.7302 - val_accuracy: 0.7950\n",
      "\n",
      "Epoch 00050: val_accuracy did not improve from 0.83000\n",
      "Epoch 51/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.2077 - accuracy: 0.9275 - val_loss: 0.5869 - val_accuracy: 0.8000\n",
      "\n",
      "Epoch 00051: val_accuracy did not improve from 0.83000\n",
      "Epoch 52/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.1430 - accuracy: 0.9688 - val_loss: 0.8967 - val_accuracy: 0.8050\n",
      "\n",
      "Epoch 00052: val_accuracy did not improve from 0.83000\n",
      "Epoch 53/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.1533 - accuracy: 0.9663 - val_loss: 0.7509 - val_accuracy: 0.8100\n",
      "\n",
      "Epoch 00053: val_accuracy did not improve from 0.83000\n",
      "Epoch 54/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.2191 - accuracy: 0.9463 - val_loss: 0.9388 - val_accuracy: 0.7400\n",
      "\n",
      "Epoch 00054: val_accuracy did not improve from 0.83000\n",
      "Epoch 55/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.5003 - accuracy: 0.7412 - val_loss: 0.5892 - val_accuracy: 0.6850\n",
      "\n",
      "Epoch 00055: val_accuracy did not improve from 0.83000\n",
      "Epoch 56/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.5221 - accuracy: 0.7138 - val_loss: 0.6090 - val_accuracy: 0.6750\n",
      "\n",
      "Epoch 00056: val_accuracy did not improve from 0.83000\n",
      "Epoch 57/2000\n",
      "800/800 [==============================] - 8s 9ms/step - loss: 0.5406 - accuracy: 0.6712 - val_loss: 0.6068 - val_accuracy: 0.6750\n",
      "\n",
      "Epoch 00057: val_accuracy did not improve from 0.83000\n",
      "Epoch 58/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.5131 - accuracy: 0.7262 - val_loss: 0.6096 - val_accuracy: 0.7050\n",
      "\n",
      "Epoch 00058: val_accuracy did not improve from 0.83000\n",
      "Epoch 59/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.4343 - accuracy: 0.7925 - val_loss: 0.6106 - val_accuracy: 0.7250\n",
      "\n",
      "Epoch 00059: val_accuracy did not improve from 0.83000\n",
      "Epoch 60/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.3534 - accuracy: 0.8575 - val_loss: 0.6666 - val_accuracy: 0.7400\n",
      "\n",
      "Epoch 00060: val_accuracy did not improve from 0.83000\n",
      "Epoch 61/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.2257 - accuracy: 0.9325 - val_loss: 0.6911 - val_accuracy: 0.7800\n",
      "\n",
      "Epoch 00061: val_accuracy did not improve from 0.83000\n",
      "Epoch 62/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.1507 - accuracy: 0.9663 - val_loss: 0.8859 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00062: val_accuracy did not improve from 0.83000\n",
      "Epoch 63/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.1284 - accuracy: 0.9762 - val_loss: 1.1760 - val_accuracy: 0.7300\n",
      "\n",
      "Epoch 00063: val_accuracy did not improve from 0.83000\n",
      "Epoch 64/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.5115 - accuracy: 0.8388 - val_loss: 0.5910 - val_accuracy: 0.6550\n",
      "\n",
      "Epoch 00064: val_accuracy did not improve from 0.83000\n",
      "Epoch 65/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.4912 - accuracy: 0.7450 - val_loss: 0.5664 - val_accuracy: 0.6600\n",
      "\n",
      "Epoch 00065: val_accuracy did not improve from 0.83000\n",
      "Epoch 66/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.3493 - accuracy: 0.8700 - val_loss: 0.7682 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00066: val_accuracy did not improve from 0.83000\n",
      "Epoch 67/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.2368 - accuracy: 0.9625 - val_loss: 0.9139 - val_accuracy: 0.7600\n",
      "\n",
      "Epoch 00067: val_accuracy did not improve from 0.83000\n",
      "Epoch 68/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.1830 - accuracy: 0.9613 - val_loss: 0.9080 - val_accuracy: 0.7450\n",
      "\n",
      "Epoch 00068: val_accuracy did not improve from 0.83000\n",
      "Epoch 69/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.2437 - accuracy: 0.9425 - val_loss: 0.8720 - val_accuracy: 0.7300\n",
      "\n",
      "Epoch 00069: val_accuracy did not improve from 0.83000\n",
      "Epoch 70/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.1852 - accuracy: 0.9550 - val_loss: 0.7778 - val_accuracy: 0.7550\n",
      "\n",
      "Epoch 00070: val_accuracy did not improve from 0.83000\n",
      "Epoch 71/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.1249 - accuracy: 0.9750 - val_loss: 0.8399 - val_accuracy: 0.7550\n",
      "\n",
      "Epoch 00071: val_accuracy did not improve from 0.83000\n",
      "Epoch 72/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.1052 - accuracy: 0.9787 - val_loss: 0.8568 - val_accuracy: 0.7850\n",
      "\n",
      "Epoch 00072: val_accuracy did not improve from 0.83000\n",
      "Epoch 73/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.1239 - accuracy: 0.9775 - val_loss: 0.8774 - val_accuracy: 0.7850\n",
      "\n",
      "Epoch 00073: val_accuracy did not improve from 0.83000\n",
      "Epoch 74/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.1101 - accuracy: 0.9787 - val_loss: 0.8582 - val_accuracy: 0.7800\n",
      "\n",
      "Epoch 00074: val_accuracy did not improve from 0.83000\n",
      "Epoch 75/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.0943 - accuracy: 0.9825 - val_loss: 0.9858 - val_accuracy: 0.7500\n",
      "\n",
      "Epoch 00075: val_accuracy did not improve from 0.83000\n",
      "Epoch 76/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.1033 - accuracy: 0.9800 - val_loss: 0.9525 - val_accuracy: 0.7600\n",
      "\n",
      "Epoch 00076: val_accuracy did not improve from 0.83000\n",
      "Epoch 77/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 8s 10ms/step - loss: 0.1036 - accuracy: 0.9787 - val_loss: 0.9556 - val_accuracy: 0.7550\n",
      "\n",
      "Epoch 00077: val_accuracy did not improve from 0.83000\n",
      "Epoch 78/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.1085 - accuracy: 0.9800 - val_loss: 0.9532 - val_accuracy: 0.7550\n",
      "\n",
      "Epoch 00078: val_accuracy did not improve from 0.83000\n",
      "Epoch 79/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.1004 - accuracy: 0.9800 - val_loss: 0.9583 - val_accuracy: 0.7550\n",
      "\n",
      "Epoch 00079: val_accuracy did not improve from 0.83000\n",
      "Epoch 80/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0967 - accuracy: 0.9812 - val_loss: 0.9522 - val_accuracy: 0.7600\n",
      "\n",
      "Epoch 00080: val_accuracy did not improve from 0.83000\n",
      "Epoch 81/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.0896 - accuracy: 0.9812 - val_loss: 0.9494 - val_accuracy: 0.7600\n",
      "\n",
      "Epoch 00081: val_accuracy did not improve from 0.83000\n",
      "Epoch 82/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0959 - accuracy: 0.9812 - val_loss: 0.9586 - val_accuracy: 0.7600\n",
      "\n",
      "Epoch 00082: val_accuracy did not improve from 0.83000\n",
      "Epoch 83/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0987 - accuracy: 0.9812 - val_loss: 0.9808 - val_accuracy: 0.7550\n",
      "\n",
      "Epoch 00083: val_accuracy did not improve from 0.83000\n",
      "Epoch 84/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0941 - accuracy: 0.9812 - val_loss: 0.9950 - val_accuracy: 0.7550\n",
      "\n",
      "Epoch 00084: val_accuracy did not improve from 0.83000\n",
      "Epoch 85/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0967 - accuracy: 0.9812 - val_loss: 0.9937 - val_accuracy: 0.7550\n",
      "\n",
      "Epoch 00085: val_accuracy did not improve from 0.83000\n",
      "Epoch 86/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0940 - accuracy: 0.9812 - val_loss: 0.9876 - val_accuracy: 0.7550\n",
      "\n",
      "Epoch 00086: val_accuracy did not improve from 0.83000\n",
      "Epoch 87/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.0962 - accuracy: 0.9812 - val_loss: 0.9990 - val_accuracy: 0.7550\n",
      "\n",
      "Epoch 00087: val_accuracy did not improve from 0.83000\n",
      "Epoch 88/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0953 - accuracy: 0.9812 - val_loss: 0.9873 - val_accuracy: 0.7550\n",
      "\n",
      "Epoch 00088: val_accuracy did not improve from 0.83000\n",
      "Epoch 89/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0994 - accuracy: 0.9812 - val_loss: 0.9686 - val_accuracy: 0.7550\n",
      "\n",
      "Epoch 00089: val_accuracy did not improve from 0.83000\n",
      "Epoch 90/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0980 - accuracy: 0.9812 - val_loss: 0.9601 - val_accuracy: 0.7550\n",
      "\n",
      "Epoch 00090: val_accuracy did not improve from 0.83000\n",
      "Epoch 91/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0998 - accuracy: 0.9812 - val_loss: 0.9761 - val_accuracy: 0.7550\n",
      "\n",
      "Epoch 00091: val_accuracy did not improve from 0.83000\n",
      "Epoch 92/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.1022 - accuracy: 0.9812 - val_loss: 0.9908 - val_accuracy: 0.7550\n",
      "\n",
      "Epoch 00092: val_accuracy did not improve from 0.83000\n",
      "Epoch 93/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0971 - accuracy: 0.9812 - val_loss: 0.9585 - val_accuracy: 0.7550\n",
      "\n",
      "Epoch 00093: val_accuracy did not improve from 0.83000\n",
      "Epoch 94/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0899 - accuracy: 0.9812 - val_loss: 0.9787 - val_accuracy: 0.7550\n",
      "\n",
      "Epoch 00094: val_accuracy did not improve from 0.83000\n",
      "Epoch 95/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0974 - accuracy: 0.9812 - val_loss: 0.9950 - val_accuracy: 0.7550\n",
      "\n",
      "Epoch 00095: val_accuracy did not improve from 0.83000\n",
      "Epoch 96/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0987 - accuracy: 0.9812 - val_loss: 0.9869 - val_accuracy: 0.7550\n",
      "\n",
      "Epoch 00096: val_accuracy did not improve from 0.83000\n",
      "Epoch 97/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.1022 - accuracy: 0.9812 - val_loss: 0.9905 - val_accuracy: 0.7550\n",
      "\n",
      "Epoch 00097: val_accuracy did not improve from 0.83000\n",
      "Epoch 98/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0920 - accuracy: 0.9812 - val_loss: 0.9743 - val_accuracy: 0.7550\n",
      "\n",
      "Epoch 00098: val_accuracy did not improve from 0.83000\n",
      "Epoch 99/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0947 - accuracy: 0.9812 - val_loss: 0.9880 - val_accuracy: 0.7550\n",
      "\n",
      "Epoch 00099: val_accuracy did not improve from 0.83000\n",
      "Epoch 100/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0906 - accuracy: 0.9812 - val_loss: 1.0063 - val_accuracy: 0.7550\n",
      "\n",
      "Epoch 00100: val_accuracy did not improve from 0.83000\n",
      "Epoch 101/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.1028 - accuracy: 0.9812 - val_loss: 0.9891 - val_accuracy: 0.7550\n",
      "\n",
      "Epoch 00101: val_accuracy did not improve from 0.83000\n",
      "Epoch 102/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0930 - accuracy: 0.9812 - val_loss: 0.9810 - val_accuracy: 0.7550\n",
      "\n",
      "Epoch 00102: val_accuracy did not improve from 0.83000\n",
      "Epoch 103/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0944 - accuracy: 0.9812 - val_loss: 0.9815 - val_accuracy: 0.7550\n",
      "\n",
      "Epoch 00103: val_accuracy did not improve from 0.83000\n",
      "Epoch 104/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0947 - accuracy: 0.9812 - val_loss: 0.9777 - val_accuracy: 0.7600\n",
      "\n",
      "Epoch 00104: val_accuracy did not improve from 0.83000\n",
      "Epoch 105/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0942 - accuracy: 0.9812 - val_loss: 0.9899 - val_accuracy: 0.7550\n",
      "\n",
      "Epoch 00105: val_accuracy did not improve from 0.83000\n",
      "Epoch 106/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0998 - accuracy: 0.9812 - val_loss: 1.0078 - val_accuracy: 0.7550\n",
      "\n",
      "Epoch 00106: val_accuracy did not improve from 0.83000\n",
      "Epoch 107/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0913 - accuracy: 0.9812 - val_loss: 0.9941 - val_accuracy: 0.7550\n",
      "\n",
      "Epoch 00107: val_accuracy did not improve from 0.83000\n",
      "Epoch 108/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0906 - accuracy: 0.9812 - val_loss: 0.9915 - val_accuracy: 0.7550\n",
      "\n",
      "Epoch 00108: val_accuracy did not improve from 0.83000\n",
      "Epoch 109/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0906 - accuracy: 0.9812 - val_loss: 1.0111 - val_accuracy: 0.7550\n",
      "\n",
      "Epoch 00109: val_accuracy did not improve from 0.83000\n",
      "Epoch 110/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0934 - accuracy: 0.9812 - val_loss: 0.9816 - val_accuracy: 0.7550\n",
      "\n",
      "Epoch 00110: val_accuracy did not improve from 0.83000\n",
      "Epoch 111/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0938 - accuracy: 0.9812 - val_loss: 0.9677 - val_accuracy: 0.7550\n",
      "\n",
      "Epoch 00111: val_accuracy did not improve from 0.83000\n",
      "Epoch 112/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0947 - accuracy: 0.9812 - val_loss: 0.9662 - val_accuracy: 0.7600\n",
      "\n",
      "Epoch 00112: val_accuracy did not improve from 0.83000\n",
      "Epoch 113/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0969 - accuracy: 0.9812 - val_loss: 0.9978 - val_accuracy: 0.7600\n",
      "\n",
      "Epoch 00113: val_accuracy did not improve from 0.83000\n",
      "Epoch 114/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0971 - accuracy: 0.9812 - val_loss: 0.9583 - val_accuracy: 0.7600\n",
      "\n",
      "Epoch 00114: val_accuracy did not improve from 0.83000\n",
      "Epoch 115/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0932 - accuracy: 0.9812 - val_loss: 0.9584 - val_accuracy: 0.7600\n",
      "\n",
      "Epoch 00115: val_accuracy did not improve from 0.83000\n",
      "Epoch 116/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0901 - accuracy: 0.9812 - val_loss: 0.9687 - val_accuracy: 0.7600\n",
      "\n",
      "Epoch 00116: val_accuracy did not improve from 0.83000\n",
      "Epoch 117/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0935 - accuracy: 0.9812 - val_loss: 0.9792 - val_accuracy: 0.7600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00117: val_accuracy did not improve from 0.83000\n",
      "Epoch 118/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0924 - accuracy: 0.9812 - val_loss: 0.9664 - val_accuracy: 0.7600\n",
      "\n",
      "Epoch 00118: val_accuracy did not improve from 0.83000\n",
      "Epoch 119/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0950 - accuracy: 0.9812 - val_loss: 0.9669 - val_accuracy: 0.7600\n",
      "\n",
      "Epoch 00119: val_accuracy did not improve from 0.83000\n",
      "Epoch 120/2000\n",
      "800/800 [==============================] - 8s 9ms/step - loss: 0.0919 - accuracy: 0.9812 - val_loss: 0.9564 - val_accuracy: 0.7600\n",
      "\n",
      "Epoch 00120: val_accuracy did not improve from 0.83000\n",
      "Epoch 121/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.0886 - accuracy: 0.9812 - val_loss: 0.9671 - val_accuracy: 0.7600\n",
      "\n",
      "Epoch 00121: val_accuracy did not improve from 0.83000\n",
      "Epoch 122/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0949 - accuracy: 0.9812 - val_loss: 0.9757 - val_accuracy: 0.7600\n",
      "\n",
      "Epoch 00122: val_accuracy did not improve from 0.83000\n",
      "Epoch 123/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.0939 - accuracy: 0.9812 - val_loss: 0.9949 - val_accuracy: 0.7550\n",
      "\n",
      "Epoch 00123: val_accuracy did not improve from 0.83000\n",
      "Epoch 124/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0960 - accuracy: 0.9812 - val_loss: 0.9636 - val_accuracy: 0.7600\n",
      "\n",
      "Epoch 00124: val_accuracy did not improve from 0.83000\n",
      "Epoch 125/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0950 - accuracy: 0.9812 - val_loss: 0.9537 - val_accuracy: 0.7600\n",
      "\n",
      "Epoch 00125: val_accuracy did not improve from 0.83000\n",
      "Epoch 126/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0944 - accuracy: 0.9812 - val_loss: 0.9653 - val_accuracy: 0.7600\n",
      "\n",
      "Epoch 00126: val_accuracy did not improve from 0.83000\n",
      "Epoch 127/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0931 - accuracy: 0.9812 - val_loss: 0.9807 - val_accuracy: 0.7600\n",
      "\n",
      "Epoch 00127: val_accuracy did not improve from 0.83000\n",
      "Epoch 128/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0967 - accuracy: 0.9812 - val_loss: 0.9910 - val_accuracy: 0.7600\n",
      "\n",
      "Epoch 00128: val_accuracy did not improve from 0.83000\n",
      "Epoch 129/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0912 - accuracy: 0.9812 - val_loss: 0.9955 - val_accuracy: 0.7600\n",
      "\n",
      "Epoch 00129: val_accuracy did not improve from 0.83000\n",
      "Epoch 130/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0981 - accuracy: 0.9812 - val_loss: 1.0082 - val_accuracy: 0.7600\n",
      "\n",
      "Epoch 00130: val_accuracy did not improve from 0.83000\n",
      "Epoch 131/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0961 - accuracy: 0.9812 - val_loss: 0.9823 - val_accuracy: 0.7600\n",
      "\n",
      "Epoch 00131: val_accuracy did not improve from 0.83000\n",
      "Epoch 132/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0942 - accuracy: 0.9812 - val_loss: 0.9784 - val_accuracy: 0.7600\n",
      "\n",
      "Epoch 00132: val_accuracy did not improve from 0.83000\n",
      "Epoch 133/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.0958 - accuracy: 0.9812 - val_loss: 0.9899 - val_accuracy: 0.7600\n",
      "\n",
      "Epoch 00133: val_accuracy did not improve from 0.83000\n",
      "Epoch 134/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0974 - accuracy: 0.9812 - val_loss: 0.9792 - val_accuracy: 0.7600\n",
      "\n",
      "Epoch 00134: val_accuracy did not improve from 0.83000\n",
      "Epoch 135/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0943 - accuracy: 0.9812 - val_loss: 0.9935 - val_accuracy: 0.7550\n",
      "\n",
      "Epoch 00135: val_accuracy did not improve from 0.83000\n",
      "Epoch 136/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.0891 - accuracy: 0.9812 - val_loss: 0.9916 - val_accuracy: 0.7550\n",
      "\n",
      "Epoch 00136: val_accuracy did not improve from 0.83000\n",
      "Epoch 137/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.0871 - accuracy: 0.9825 - val_loss: 1.0259 - val_accuracy: 0.7500\n",
      "\n",
      "Epoch 00137: val_accuracy did not improve from 0.83000\n",
      "Epoch 138/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0863 - accuracy: 0.9825 - val_loss: 1.0269 - val_accuracy: 0.7600\n",
      "\n",
      "Epoch 00138: val_accuracy did not improve from 0.83000\n",
      "Epoch 139/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0817 - accuracy: 0.9850 - val_loss: 0.9764 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00139: val_accuracy did not improve from 0.83000\n",
      "Epoch 140/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0770 - accuracy: 0.9862 - val_loss: 1.0252 - val_accuracy: 0.7650\n",
      "\n",
      "Epoch 00140: val_accuracy did not improve from 0.83000\n",
      "Epoch 141/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0771 - accuracy: 0.9862 - val_loss: 1.0345 - val_accuracy: 0.7650\n",
      "\n",
      "Epoch 00141: val_accuracy did not improve from 0.83000\n",
      "Epoch 142/2000\n",
      "800/800 [==============================] - 8s 9ms/step - loss: 0.0798 - accuracy: 0.9862 - val_loss: 1.0650 - val_accuracy: 0.7650\n",
      "\n",
      "Epoch 00142: val_accuracy did not improve from 0.83000\n",
      "Epoch 143/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0741 - accuracy: 0.9862 - val_loss: 1.0440 - val_accuracy: 0.7650\n",
      "\n",
      "Epoch 00143: val_accuracy did not improve from 0.83000\n",
      "Epoch 144/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.0819 - accuracy: 0.9862 - val_loss: 1.0149 - val_accuracy: 0.7650\n",
      "\n",
      "Epoch 00144: val_accuracy did not improve from 0.83000\n",
      "Epoch 145/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0733 - accuracy: 0.9862 - val_loss: 1.0297 - val_accuracy: 0.7650\n",
      "\n",
      "Epoch 00145: val_accuracy did not improve from 0.83000\n",
      "Epoch 146/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0774 - accuracy: 0.9862 - val_loss: 1.0402 - val_accuracy: 0.7650\n",
      "\n",
      "Epoch 00146: val_accuracy did not improve from 0.83000\n",
      "Epoch 147/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0799 - accuracy: 0.9862 - val_loss: 1.0398 - val_accuracy: 0.7650\n",
      "\n",
      "Epoch 00147: val_accuracy did not improve from 0.83000\n",
      "Epoch 148/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0679 - accuracy: 0.9862 - val_loss: 1.0564 - val_accuracy: 0.7650\n",
      "\n",
      "Epoch 00148: val_accuracy did not improve from 0.83000\n",
      "Epoch 149/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0743 - accuracy: 0.9862 - val_loss: 1.0635 - val_accuracy: 0.7650\n",
      "\n",
      "Epoch 00149: val_accuracy did not improve from 0.83000\n",
      "Epoch 150/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0765 - accuracy: 0.9862 - val_loss: 1.0352 - val_accuracy: 0.7650\n",
      "\n",
      "Epoch 00150: val_accuracy did not improve from 0.83000\n",
      "Epoch 151/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0799 - accuracy: 0.9862 - val_loss: 0.9920 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00151: val_accuracy did not improve from 0.83000\n",
      "Epoch 152/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0717 - accuracy: 0.9862 - val_loss: 1.0100 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00152: val_accuracy did not improve from 0.83000\n",
      "Epoch 153/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0753 - accuracy: 0.9862 - val_loss: 1.0051 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00153: val_accuracy did not improve from 0.83000\n",
      "Epoch 154/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0752 - accuracy: 0.9875 - val_loss: 1.0246 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00154: val_accuracy did not improve from 0.83000\n",
      "Epoch 155/2000\n",
      "800/800 [==============================] - 8s 9ms/step - loss: 0.0702 - accuracy: 0.9875 - val_loss: 1.0478 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00155: val_accuracy did not improve from 0.83000\n",
      "Epoch 156/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0755 - accuracy: 0.9875 - val_loss: 1.0396 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00156: val_accuracy did not improve from 0.83000\n",
      "Epoch 157/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0667 - accuracy: 0.9887 - val_loss: 1.0486 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00157: val_accuracy did not improve from 0.83000\n",
      "Epoch 158/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0634 - accuracy: 0.9887 - val_loss: 1.0517 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00158: val_accuracy did not improve from 0.83000\n",
      "Epoch 159/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.0669 - accuracy: 0.9887 - val_loss: 1.0811 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00159: val_accuracy did not improve from 0.83000\n",
      "Epoch 160/2000\n",
      "800/800 [==============================] - 8s 9ms/step - loss: 0.0637 - accuracy: 0.9887 - val_loss: 1.0835 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00160: val_accuracy did not improve from 0.83000\n",
      "Epoch 161/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.0660 - accuracy: 0.9887 - val_loss: 1.0852 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00161: val_accuracy did not improve from 0.83000\n",
      "Epoch 162/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.0681 - accuracy: 0.9887 - val_loss: 1.0611 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00162: val_accuracy did not improve from 0.83000\n",
      "Epoch 163/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.0572 - accuracy: 0.9900 - val_loss: 1.0686 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00163: val_accuracy did not improve from 0.83000\n",
      "Epoch 164/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.0595 - accuracy: 0.9900 - val_loss: 1.1023 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00164: val_accuracy did not improve from 0.83000\n",
      "Epoch 165/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.0554 - accuracy: 0.9900 - val_loss: 1.1157 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00165: val_accuracy did not improve from 0.83000\n",
      "Epoch 166/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.0560 - accuracy: 0.9900 - val_loss: 1.1101 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00166: val_accuracy did not improve from 0.83000\n",
      "Epoch 167/2000\n",
      "800/800 [==============================] - 9s 12ms/step - loss: 0.0564 - accuracy: 0.9900 - val_loss: 1.0922 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00167: val_accuracy did not improve from 0.83000\n",
      "Epoch 168/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.0635 - accuracy: 0.9900 - val_loss: 1.0785 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00168: val_accuracy did not improve from 0.83000\n",
      "Epoch 169/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0605 - accuracy: 0.9900 - val_loss: 1.1043 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00169: val_accuracy did not improve from 0.83000\n",
      "Epoch 170/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.0574 - accuracy: 0.9900 - val_loss: 1.0921 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00170: val_accuracy did not improve from 0.83000\n",
      "Epoch 171/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.0611 - accuracy: 0.9900 - val_loss: 1.0993 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00171: val_accuracy did not improve from 0.83000\n",
      "Epoch 172/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0613 - accuracy: 0.9900 - val_loss: 1.0863 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00172: val_accuracy did not improve from 0.83000\n",
      "Epoch 173/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0643 - accuracy: 0.9900 - val_loss: 1.0821 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00173: val_accuracy did not improve from 0.83000\n",
      "Epoch 174/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0600 - accuracy: 0.9900 - val_loss: 1.1103 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00174: val_accuracy did not improve from 0.83000\n",
      "Epoch 175/2000\n",
      "800/800 [==============================] - 8s 9ms/step - loss: 0.0553 - accuracy: 0.9900 - val_loss: 1.1084 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00175: val_accuracy did not improve from 0.83000\n",
      "Epoch 176/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0594 - accuracy: 0.9900 - val_loss: 1.1102 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00176: val_accuracy did not improve from 0.83000\n",
      "Epoch 177/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.0569 - accuracy: 0.9900 - val_loss: 1.1233 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00177: val_accuracy did not improve from 0.83000\n",
      "Epoch 178/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0561 - accuracy: 0.9900 - val_loss: 1.1118 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00178: val_accuracy did not improve from 0.83000\n",
      "Epoch 179/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0610 - accuracy: 0.9900 - val_loss: 1.0840 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00179: val_accuracy did not improve from 0.83000\n",
      "Epoch 180/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0581 - accuracy: 0.9900 - val_loss: 1.0822 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00180: val_accuracy did not improve from 0.83000\n",
      "Epoch 181/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0604 - accuracy: 0.9900 - val_loss: 1.0987 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00181: val_accuracy did not improve from 0.83000\n",
      "Epoch 182/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0582 - accuracy: 0.9900 - val_loss: 1.1050 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00182: val_accuracy did not improve from 0.83000\n",
      "Epoch 183/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0642 - accuracy: 0.9900 - val_loss: 1.0880 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00183: val_accuracy did not improve from 0.83000\n",
      "Epoch 184/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0536 - accuracy: 0.9900 - val_loss: 1.1045 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00184: val_accuracy did not improve from 0.83000\n",
      "Epoch 185/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0614 - accuracy: 0.9900 - val_loss: 1.1221 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00185: val_accuracy did not improve from 0.83000\n",
      "Epoch 186/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0586 - accuracy: 0.9900 - val_loss: 1.1225 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00186: val_accuracy did not improve from 0.83000\n",
      "Epoch 187/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0580 - accuracy: 0.9900 - val_loss: 1.0999 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00187: val_accuracy did not improve from 0.83000\n",
      "Epoch 188/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0564 - accuracy: 0.9900 - val_loss: 1.0696 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00188: val_accuracy did not improve from 0.83000\n",
      "Epoch 189/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0609 - accuracy: 0.9900 - val_loss: 1.0621 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00189: val_accuracy did not improve from 0.83000\n",
      "Epoch 190/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0618 - accuracy: 0.9900 - val_loss: 1.0874 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00190: val_accuracy did not improve from 0.83000\n",
      "Epoch 191/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0597 - accuracy: 0.9900 - val_loss: 1.0923 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00191: val_accuracy did not improve from 0.83000\n",
      "Epoch 192/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0559 - accuracy: 0.9900 - val_loss: 1.1026 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00192: val_accuracy did not improve from 0.83000\n",
      "Epoch 193/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0580 - accuracy: 0.9900 - val_loss: 1.1007 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00193: val_accuracy did not improve from 0.83000\n",
      "Epoch 194/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0586 - accuracy: 0.9900 - val_loss: 1.1015 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00194: val_accuracy did not improve from 0.83000\n",
      "Epoch 195/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.0596 - accuracy: 0.9900 - val_loss: 1.0936 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00195: val_accuracy did not improve from 0.83000\n",
      "Epoch 196/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.0580 - accuracy: 0.9900 - val_loss: 1.1124 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00196: val_accuracy did not improve from 0.83000\n",
      "Epoch 197/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.0539 - accuracy: 0.9900 - val_loss: 1.0971 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00197: val_accuracy did not improve from 0.83000\n",
      "Epoch 198/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0613 - accuracy: 0.9900 - val_loss: 1.0681 - val_accuracy: 0.7700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00198: val_accuracy did not improve from 0.83000\n",
      "Epoch 199/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0620 - accuracy: 0.9900 - val_loss: 1.0781 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00199: val_accuracy did not improve from 0.83000\n",
      "Epoch 200/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0572 - accuracy: 0.9900 - val_loss: 1.0997 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00200: val_accuracy did not improve from 0.83000\n",
      "Epoch 201/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0559 - accuracy: 0.9900 - val_loss: 1.1261 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00201: val_accuracy did not improve from 0.83000\n",
      "Epoch 202/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0563 - accuracy: 0.9900 - val_loss: 1.1140 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00202: val_accuracy did not improve from 0.83000\n",
      "Epoch 203/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0581 - accuracy: 0.9900 - val_loss: 1.1138 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00203: val_accuracy did not improve from 0.83000\n",
      "Epoch 204/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0625 - accuracy: 0.9900 - val_loss: 1.1109 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00204: val_accuracy did not improve from 0.83000\n",
      "Epoch 205/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0577 - accuracy: 0.9900 - val_loss: 1.1099 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00205: val_accuracy did not improve from 0.83000\n",
      "Epoch 206/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0581 - accuracy: 0.9900 - val_loss: 1.0956 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00206: val_accuracy did not improve from 0.83000\n",
      "Epoch 207/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.0588 - accuracy: 0.9900 - val_loss: 1.0777 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00207: val_accuracy did not improve from 0.83000\n",
      "Epoch 208/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0616 - accuracy: 0.9900 - val_loss: 1.0721 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00208: val_accuracy did not improve from 0.83000\n",
      "Epoch 209/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0581 - accuracy: 0.9900 - val_loss: 1.0964 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00209: val_accuracy did not improve from 0.83000\n",
      "Epoch 210/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0585 - accuracy: 0.9900 - val_loss: 1.0720 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00210: val_accuracy did not improve from 0.83000\n",
      "Epoch 211/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0660 - accuracy: 0.9887 - val_loss: 1.0876 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00211: val_accuracy did not improve from 0.83000\n",
      "Epoch 212/2000\n",
      "800/800 [==============================] - 8s 9ms/step - loss: 0.0586 - accuracy: 0.9900 - val_loss: 1.0620 - val_accuracy: 0.7800\n",
      "\n",
      "Epoch 00212: val_accuracy did not improve from 0.83000\n",
      "Epoch 213/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0617 - accuracy: 0.9887 - val_loss: 0.9973 - val_accuracy: 0.7850\n",
      "\n",
      "Epoch 00213: val_accuracy did not improve from 0.83000\n",
      "Epoch 214/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0658 - accuracy: 0.9887 - val_loss: 0.9743 - val_accuracy: 0.7900\n",
      "\n",
      "Epoch 00214: val_accuracy did not improve from 0.83000\n",
      "Epoch 215/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0546 - accuracy: 0.9912 - val_loss: 0.9991 - val_accuracy: 0.7900\n",
      "\n",
      "Epoch 00215: val_accuracy did not improve from 0.83000\n",
      "Epoch 216/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0563 - accuracy: 0.9912 - val_loss: 1.0246 - val_accuracy: 0.7900\n",
      "\n",
      "Epoch 00216: val_accuracy did not improve from 0.83000\n",
      "Epoch 217/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0524 - accuracy: 0.9912 - val_loss: 1.0177 - val_accuracy: 0.7900\n",
      "\n",
      "Epoch 00217: val_accuracy did not improve from 0.83000\n",
      "Epoch 218/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0484 - accuracy: 0.9912 - val_loss: 1.0309 - val_accuracy: 0.7900\n",
      "\n",
      "Epoch 00218: val_accuracy did not improve from 0.83000\n",
      "Epoch 219/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0528 - accuracy: 0.9912 - val_loss: 1.0546 - val_accuracy: 0.7900\n",
      "\n",
      "Epoch 00219: val_accuracy did not improve from 0.83000\n",
      "Epoch 220/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0546 - accuracy: 0.9912 - val_loss: 1.0491 - val_accuracy: 0.7900\n",
      "\n",
      "Epoch 00220: val_accuracy did not improve from 0.83000\n",
      "Epoch 221/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0496 - accuracy: 0.9912 - val_loss: 1.0547 - val_accuracy: 0.7900\n",
      "\n",
      "Epoch 00221: val_accuracy did not improve from 0.83000\n",
      "Epoch 222/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0470 - accuracy: 0.9912 - val_loss: 1.0472 - val_accuracy: 0.7900\n",
      "\n",
      "Epoch 00222: val_accuracy did not improve from 0.83000\n",
      "Epoch 223/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0563 - accuracy: 0.9912 - val_loss: 1.0703 - val_accuracy: 0.7900\n",
      "\n",
      "Epoch 00223: val_accuracy did not improve from 0.83000\n",
      "Epoch 224/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0558 - accuracy: 0.9912 - val_loss: 1.0097 - val_accuracy: 0.7900\n",
      "\n",
      "Epoch 00224: val_accuracy did not improve from 0.83000\n",
      "Epoch 225/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0527 - accuracy: 0.9912 - val_loss: 1.0305 - val_accuracy: 0.7900\n",
      "\n",
      "Epoch 00225: val_accuracy did not improve from 0.83000\n",
      "Epoch 226/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0551 - accuracy: 0.9912 - val_loss: 1.0221 - val_accuracy: 0.7900\n",
      "\n",
      "Epoch 00226: val_accuracy did not improve from 0.83000\n",
      "Epoch 227/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0519 - accuracy: 0.9912 - val_loss: 1.0336 - val_accuracy: 0.7900\n",
      "\n",
      "Epoch 00227: val_accuracy did not improve from 0.83000\n",
      "Epoch 228/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0510 - accuracy: 0.9912 - val_loss: 1.0529 - val_accuracy: 0.7900\n",
      "\n",
      "Epoch 00228: val_accuracy did not improve from 0.83000\n",
      "Epoch 229/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.0529 - accuracy: 0.9912 - val_loss: 1.0552 - val_accuracy: 0.7900\n",
      "\n",
      "Epoch 00229: val_accuracy did not improve from 0.83000\n",
      "Epoch 230/2000\n",
      "800/800 [==============================] - 10s 12ms/step - loss: 0.0529 - accuracy: 0.9912 - val_loss: 1.0398 - val_accuracy: 0.7900\n",
      "\n",
      "Epoch 00230: val_accuracy did not improve from 0.83000\n",
      "Epoch 231/2000\n",
      "800/800 [==============================] - 12s 15ms/step - loss: 0.0514 - accuracy: 0.9912 - val_loss: 1.0315 - val_accuracy: 0.7900\n",
      "\n",
      "Epoch 00231: val_accuracy did not improve from 0.83000\n",
      "Epoch 232/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.0485 - accuracy: 0.9912 - val_loss: 1.0519 - val_accuracy: 0.7900\n",
      "\n",
      "Epoch 00232: val_accuracy did not improve from 0.83000\n",
      "Epoch 233/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.0511 - accuracy: 0.9912 - val_loss: 1.0561 - val_accuracy: 0.7900\n",
      "\n",
      "Epoch 00233: val_accuracy did not improve from 0.83000\n",
      "Epoch 234/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.0518 - accuracy: 0.9912 - val_loss: 1.0351 - val_accuracy: 0.7900\n",
      "\n",
      "Epoch 00234: val_accuracy did not improve from 0.83000\n",
      "Epoch 235/2000\n",
      "800/800 [==============================] - 8s 9ms/step - loss: 0.0568 - accuracy: 0.9912 - val_loss: 1.0702 - val_accuracy: 0.7850\n",
      "\n",
      "Epoch 00235: val_accuracy did not improve from 0.83000\n",
      "Epoch 236/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.0603 - accuracy: 0.9900 - val_loss: 0.8535 - val_accuracy: 0.8100\n",
      "\n",
      "Epoch 00236: val_accuracy did not improve from 0.83000\n",
      "Epoch 237/2000\n",
      "800/800 [==============================] - 11s 14ms/step - loss: 0.0793 - accuracy: 0.9837 - val_loss: 0.8730 - val_accuracy: 0.8000\n",
      "\n",
      "Epoch 00237: val_accuracy did not improve from 0.83000\n",
      "Epoch 238/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.3038 - accuracy: 0.9350 - val_loss: 1.1397 - val_accuracy: 0.7150\n",
      "\n",
      "Epoch 00238: val_accuracy did not improve from 0.83000\n",
      "Epoch 239/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 10s 12ms/step - loss: 0.8436 - accuracy: 0.6100 - val_loss: 0.7391 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00239: val_accuracy did not improve from 0.83000\n",
      "Epoch 240/2000\n",
      "800/800 [==============================] - 9s 12ms/step - loss: 0.7011 - accuracy: 0.5113 - val_loss: 0.6740 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00240: val_accuracy did not improve from 0.83000\n",
      "Epoch 241/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6864 - accuracy: 0.4988 - val_loss: 0.6761 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00241: val_accuracy did not improve from 0.83000\n",
      "Epoch 242/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6889 - accuracy: 0.4888 - val_loss: 0.6767 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00242: val_accuracy did not improve from 0.83000\n",
      "Epoch 243/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6865 - accuracy: 0.4888 - val_loss: 0.6792 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00243: val_accuracy did not improve from 0.83000\n",
      "Epoch 244/2000\n",
      "800/800 [==============================] - 9s 12ms/step - loss: 0.6788 - accuracy: 0.5188 - val_loss: 0.6781 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00244: val_accuracy did not improve from 0.83000\n",
      "Epoch 245/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6796 - accuracy: 0.5100 - val_loss: 0.6745 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00245: val_accuracy did not improve from 0.83000\n",
      "Epoch 246/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6848 - accuracy: 0.4812 - val_loss: 0.6784 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00246: val_accuracy did not improve from 0.83000\n",
      "Epoch 247/2000\n",
      "800/800 [==============================] - 10s 12ms/step - loss: 0.6807 - accuracy: 0.5025 - val_loss: 0.6770 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00247: val_accuracy did not improve from 0.83000\n",
      "Epoch 248/2000\n",
      "800/800 [==============================] - 10s 13ms/step - loss: 0.6810 - accuracy: 0.5275 - val_loss: 0.6728 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00248: val_accuracy did not improve from 0.83000\n",
      "Epoch 249/2000\n",
      "800/800 [==============================] - 11s 14ms/step - loss: 0.6809 - accuracy: 0.5025 - val_loss: 0.6788 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00249: val_accuracy did not improve from 0.83000\n",
      "Epoch 250/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6810 - accuracy: 0.4938 - val_loss: 0.6765 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00250: val_accuracy did not improve from 0.83000\n",
      "Epoch 251/2000\n",
      "800/800 [==============================] - 10s 13ms/step - loss: 0.6798 - accuracy: 0.5063 - val_loss: 0.6767 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00251: val_accuracy did not improve from 0.83000\n",
      "Epoch 252/2000\n",
      "800/800 [==============================] - 10s 13ms/step - loss: 0.6795 - accuracy: 0.4875 - val_loss: 0.6752 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00252: val_accuracy did not improve from 0.83000\n",
      "Epoch 253/2000\n",
      "800/800 [==============================] - 10s 13ms/step - loss: 0.6797 - accuracy: 0.5150 - val_loss: 0.6779 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00253: val_accuracy did not improve from 0.83000\n",
      "Epoch 254/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6795 - accuracy: 0.4988 - val_loss: 0.6762 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00254: val_accuracy did not improve from 0.83000\n",
      "Epoch 255/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6792 - accuracy: 0.5063 - val_loss: 0.6770 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00255: val_accuracy did not improve from 0.83000\n",
      "Epoch 256/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6797 - accuracy: 0.5038 - val_loss: 0.6767 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00256: val_accuracy did not improve from 0.83000\n",
      "Epoch 257/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6800 - accuracy: 0.4988 - val_loss: 0.6768 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00257: val_accuracy did not improve from 0.83000\n",
      "Epoch 258/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6765 - accuracy: 0.5362 - val_loss: 0.6766 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00258: val_accuracy did not improve from 0.83000\n",
      "Epoch 259/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6774 - accuracy: 0.5412 - val_loss: 0.6761 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00259: val_accuracy did not improve from 0.83000\n",
      "Epoch 260/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6795 - accuracy: 0.5050 - val_loss: 0.6756 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00260: val_accuracy did not improve from 0.83000\n",
      "Epoch 261/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6794 - accuracy: 0.5063 - val_loss: 0.6759 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00261: val_accuracy did not improve from 0.83000\n",
      "Epoch 262/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6784 - accuracy: 0.4913 - val_loss: 0.6761 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00262: val_accuracy did not improve from 0.83000\n",
      "Epoch 263/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6812 - accuracy: 0.4638 - val_loss: 0.6783 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00263: val_accuracy did not improve from 0.83000\n",
      "Epoch 264/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6779 - accuracy: 0.5425 - val_loss: 0.6757 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00264: val_accuracy did not improve from 0.83000\n",
      "Epoch 265/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6775 - accuracy: 0.5312 - val_loss: 0.6753 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00265: val_accuracy did not improve from 0.83000\n",
      "Epoch 266/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6791 - accuracy: 0.5275 - val_loss: 0.6750 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00266: val_accuracy did not improve from 0.83000\n",
      "Epoch 267/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6787 - accuracy: 0.4975 - val_loss: 0.6759 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00267: val_accuracy did not improve from 0.83000\n",
      "Epoch 268/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6772 - accuracy: 0.5387 - val_loss: 0.6762 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00268: val_accuracy did not improve from 0.83000\n",
      "Epoch 269/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6819 - accuracy: 0.4750 - val_loss: 0.6776 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00269: val_accuracy did not improve from 0.83000\n",
      "Epoch 270/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6803 - accuracy: 0.4963 - val_loss: 0.6785 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00270: val_accuracy did not improve from 0.83000\n",
      "Epoch 271/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6790 - accuracy: 0.4875 - val_loss: 0.6765 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00271: val_accuracy did not improve from 0.83000\n",
      "Epoch 272/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6797 - accuracy: 0.4938 - val_loss: 0.6793 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00272: val_accuracy did not improve from 0.83000\n",
      "Epoch 273/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6794 - accuracy: 0.4938 - val_loss: 0.6758 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00273: val_accuracy did not improve from 0.83000\n",
      "Epoch 274/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6793 - accuracy: 0.5075 - val_loss: 0.6761 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00274: val_accuracy did not improve from 0.83000\n",
      "Epoch 275/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6779 - accuracy: 0.5200 - val_loss: 0.6784 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00275: val_accuracy did not improve from 0.83000\n",
      "Epoch 276/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6794 - accuracy: 0.5138 - val_loss: 0.6779 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00276: val_accuracy did not improve from 0.83000\n",
      "Epoch 277/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6795 - accuracy: 0.5038 - val_loss: 0.6780 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00277: val_accuracy did not improve from 0.83000\n",
      "Epoch 278/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6784 - accuracy: 0.5113 - val_loss: 0.6779 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00278: val_accuracy did not improve from 0.83000\n",
      "Epoch 279/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6794 - accuracy: 0.4850 - val_loss: 0.6765 - val_accuracy: 0.5700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00279: val_accuracy did not improve from 0.83000\n",
      "Epoch 280/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6788 - accuracy: 0.5113 - val_loss: 0.6756 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00280: val_accuracy did not improve from 0.83000\n",
      "Epoch 281/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6782 - accuracy: 0.5225 - val_loss: 0.6761 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00281: val_accuracy did not improve from 0.83000\n",
      "Epoch 282/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6794 - accuracy: 0.5000 - val_loss: 0.6774 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00282: val_accuracy did not improve from 0.83000\n",
      "Epoch 283/2000\n",
      "800/800 [==============================] - 8s 9ms/step - loss: 0.6800 - accuracy: 0.4600 - val_loss: 0.6779 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00283: val_accuracy did not improve from 0.83000\n",
      "Epoch 284/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6795 - accuracy: 0.4975 - val_loss: 0.6777 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00284: val_accuracy did not improve from 0.83000\n",
      "Epoch 285/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6787 - accuracy: 0.5013 - val_loss: 0.6772 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00285: val_accuracy did not improve from 0.83000\n",
      "Epoch 286/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6772 - accuracy: 0.5575 - val_loss: 0.6774 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00286: val_accuracy did not improve from 0.83000\n",
      "Epoch 287/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6774 - accuracy: 0.5300 - val_loss: 0.6769 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00287: val_accuracy did not improve from 0.83000\n",
      "Epoch 288/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6809 - accuracy: 0.4700 - val_loss: 0.6774 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00288: val_accuracy did not improve from 0.83000\n",
      "Epoch 289/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6793 - accuracy: 0.5050 - val_loss: 0.6747 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00289: val_accuracy did not improve from 0.83000\n",
      "Epoch 290/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6772 - accuracy: 0.5288 - val_loss: 0.6759 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00290: val_accuracy did not improve from 0.83000\n",
      "Epoch 291/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6792 - accuracy: 0.5025 - val_loss: 0.6769 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00291: val_accuracy did not improve from 0.83000\n",
      "Epoch 292/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6784 - accuracy: 0.5425 - val_loss: 0.6771 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00292: val_accuracy did not improve from 0.83000\n",
      "Epoch 293/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6804 - accuracy: 0.5038 - val_loss: 0.6771 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00293: val_accuracy did not improve from 0.83000\n",
      "Epoch 294/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6787 - accuracy: 0.4800 - val_loss: 0.6774 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00294: val_accuracy did not improve from 0.83000\n",
      "Epoch 295/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6800 - accuracy: 0.5025 - val_loss: 0.6781 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00295: val_accuracy did not improve from 0.83000\n",
      "Epoch 296/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6790 - accuracy: 0.5050 - val_loss: 0.6768 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00296: val_accuracy did not improve from 0.83000\n",
      "Epoch 297/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6789 - accuracy: 0.5100 - val_loss: 0.6767 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00297: val_accuracy did not improve from 0.83000\n",
      "Epoch 298/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6795 - accuracy: 0.5100 - val_loss: 0.6774 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00298: val_accuracy did not improve from 0.83000\n",
      "Epoch 299/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6812 - accuracy: 0.4775 - val_loss: 0.6771 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00299: val_accuracy did not improve from 0.83000\n",
      "Epoch 300/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6783 - accuracy: 0.5412 - val_loss: 0.6778 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00300: val_accuracy did not improve from 0.83000\n",
      "Epoch 301/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6779 - accuracy: 0.5500 - val_loss: 0.6771 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00301: val_accuracy did not improve from 0.83000\n",
      "Epoch 302/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6794 - accuracy: 0.5163 - val_loss: 0.6763 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00302: val_accuracy did not improve from 0.83000\n",
      "Epoch 303/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6780 - accuracy: 0.5238 - val_loss: 0.6757 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00303: val_accuracy did not improve from 0.83000\n",
      "Epoch 304/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6797 - accuracy: 0.5125 - val_loss: 0.6755 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00304: val_accuracy did not improve from 0.83000\n",
      "Epoch 305/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6796 - accuracy: 0.4775 - val_loss: 0.6767 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00305: val_accuracy did not improve from 0.83000\n",
      "Epoch 306/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6773 - accuracy: 0.5350 - val_loss: 0.6757 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00306: val_accuracy did not improve from 0.83000\n",
      "Epoch 307/2000\n",
      "800/800 [==============================] - 8s 9ms/step - loss: 0.6788 - accuracy: 0.5200 - val_loss: 0.6759 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00307: val_accuracy did not improve from 0.83000\n",
      "Epoch 308/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6791 - accuracy: 0.5400 - val_loss: 0.6759 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00308: val_accuracy did not improve from 0.83000\n",
      "Epoch 309/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6798 - accuracy: 0.5275 - val_loss: 0.6752 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00309: val_accuracy did not improve from 0.83000\n",
      "Epoch 310/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6805 - accuracy: 0.4863 - val_loss: 0.6763 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00310: val_accuracy did not improve from 0.83000\n",
      "Epoch 311/2000\n",
      "800/800 [==============================] - 11s 13ms/step - loss: 0.6784 - accuracy: 0.5238 - val_loss: 0.6761 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00311: val_accuracy did not improve from 0.83000\n",
      "Epoch 312/2000\n",
      "800/800 [==============================] - 10s 12ms/step - loss: 0.6791 - accuracy: 0.5113 - val_loss: 0.6771 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00312: val_accuracy did not improve from 0.83000\n",
      "Epoch 313/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6788 - accuracy: 0.5200 - val_loss: 0.6770 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00313: val_accuracy did not improve from 0.83000\n",
      "Epoch 314/2000\n",
      "800/800 [==============================] - 9s 12ms/step - loss: 0.6793 - accuracy: 0.5375 - val_loss: 0.6766 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00314: val_accuracy did not improve from 0.83000\n",
      "Epoch 315/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6791 - accuracy: 0.5200 - val_loss: 0.6764 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00315: val_accuracy did not improve from 0.83000\n",
      "Epoch 316/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6787 - accuracy: 0.5113 - val_loss: 0.6769 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00316: val_accuracy did not improve from 0.83000\n",
      "Epoch 317/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6789 - accuracy: 0.5288 - val_loss: 0.6775 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00317: val_accuracy did not improve from 0.83000\n",
      "Epoch 318/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6792 - accuracy: 0.4925 - val_loss: 0.6772 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00318: val_accuracy did not improve from 0.83000\n",
      "Epoch 319/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6782 - accuracy: 0.5312 - val_loss: 0.6750 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00319: val_accuracy did not improve from 0.83000\n",
      "Epoch 320/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6789 - accuracy: 0.5150 - val_loss: 0.6739 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00320: val_accuracy did not improve from 0.83000\n",
      "Epoch 321/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6808 - accuracy: 0.4913 - val_loss: 0.6755 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00321: val_accuracy did not improve from 0.83000\n",
      "Epoch 322/2000\n",
      "800/800 [==============================] - 8s 9ms/step - loss: 0.6791 - accuracy: 0.5150 - val_loss: 0.6763 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00322: val_accuracy did not improve from 0.83000\n",
      "Epoch 323/2000\n",
      "800/800 [==============================] - 8s 11ms/step - loss: 0.6790 - accuracy: 0.5113 - val_loss: 0.6762 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00323: val_accuracy did not improve from 0.83000\n",
      "Epoch 324/2000\n",
      "800/800 [==============================] - 9s 12ms/step - loss: 0.6810 - accuracy: 0.5100 - val_loss: 0.6767 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00324: val_accuracy did not improve from 0.83000\n",
      "Epoch 325/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6802 - accuracy: 0.4900 - val_loss: 0.6768 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00325: val_accuracy did not improve from 0.83000\n",
      "Epoch 326/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6792 - accuracy: 0.4850 - val_loss: 0.6765 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00326: val_accuracy did not improve from 0.83000\n",
      "Epoch 327/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6781 - accuracy: 0.5100 - val_loss: 0.6759 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00327: val_accuracy did not improve from 0.83000\n",
      "Epoch 328/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6805 - accuracy: 0.4863 - val_loss: 0.6759 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00328: val_accuracy did not improve from 0.83000\n",
      "Epoch 329/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6809 - accuracy: 0.4925 - val_loss: 0.6773 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00329: val_accuracy did not improve from 0.83000\n",
      "Epoch 330/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6790 - accuracy: 0.5050 - val_loss: 0.6784 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00330: val_accuracy did not improve from 0.83000\n",
      "Epoch 331/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6773 - accuracy: 0.5300 - val_loss: 0.6775 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00331: val_accuracy did not improve from 0.83000\n",
      "Epoch 332/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6788 - accuracy: 0.4775 - val_loss: 0.6755 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00332: val_accuracy did not improve from 0.83000\n",
      "Epoch 333/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6787 - accuracy: 0.5288 - val_loss: 0.6757 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00333: val_accuracy did not improve from 0.83000\n",
      "Epoch 334/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6796 - accuracy: 0.4425 - val_loss: 0.6775 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00334: val_accuracy did not improve from 0.83000\n",
      "Epoch 335/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6804 - accuracy: 0.4975 - val_loss: 0.6766 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00335: val_accuracy did not improve from 0.83000\n",
      "Epoch 336/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6785 - accuracy: 0.5088 - val_loss: 0.6771 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00336: val_accuracy did not improve from 0.83000\n",
      "Epoch 337/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6784 - accuracy: 0.5163 - val_loss: 0.6776 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00337: val_accuracy did not improve from 0.83000\n",
      "Epoch 338/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6793 - accuracy: 0.5000 - val_loss: 0.6782 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00338: val_accuracy did not improve from 0.83000\n",
      "Epoch 339/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6783 - accuracy: 0.5000 - val_loss: 0.6762 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00339: val_accuracy did not improve from 0.83000\n",
      "Epoch 340/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6796 - accuracy: 0.4812 - val_loss: 0.6762 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00340: val_accuracy did not improve from 0.83000\n",
      "Epoch 341/2000\n",
      "800/800 [==============================] - 8s 11ms/step - loss: 0.6788 - accuracy: 0.5238 - val_loss: 0.6768 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00341: val_accuracy did not improve from 0.83000\n",
      "Epoch 342/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6789 - accuracy: 0.5362 - val_loss: 0.6767 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00342: val_accuracy did not improve from 0.83000\n",
      "Epoch 343/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6787 - accuracy: 0.5238 - val_loss: 0.6763 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00343: val_accuracy did not improve from 0.83000\n",
      "Epoch 344/2000\n",
      "800/800 [==============================] - 8s 11ms/step - loss: 0.6787 - accuracy: 0.5537 - val_loss: 0.6751 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00344: val_accuracy did not improve from 0.83000\n",
      "Epoch 345/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6785 - accuracy: 0.4988 - val_loss: 0.6762 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00345: val_accuracy did not improve from 0.83000\n",
      "Epoch 346/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6801 - accuracy: 0.4825 - val_loss: 0.6771 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00346: val_accuracy did not improve from 0.83000\n",
      "Epoch 347/2000\n",
      "800/800 [==============================] - 10s 12ms/step - loss: 0.6788 - accuracy: 0.5362 - val_loss: 0.6774 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00347: val_accuracy did not improve from 0.83000\n",
      "Epoch 348/2000\n",
      "800/800 [==============================] - 9s 12ms/step - loss: 0.6793 - accuracy: 0.5250 - val_loss: 0.6769 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00348: val_accuracy did not improve from 0.83000\n",
      "Epoch 349/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6772 - accuracy: 0.5300 - val_loss: 0.6773 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00349: val_accuracy did not improve from 0.83000\n",
      "Epoch 350/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6793 - accuracy: 0.5138 - val_loss: 0.6786 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00350: val_accuracy did not improve from 0.83000\n",
      "Epoch 351/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6788 - accuracy: 0.5175 - val_loss: 0.6769 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00351: val_accuracy did not improve from 0.83000\n",
      "Epoch 352/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6795 - accuracy: 0.5000 - val_loss: 0.6766 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00352: val_accuracy did not improve from 0.83000\n",
      "Epoch 353/2000\n",
      "800/800 [==============================] - 10s 12ms/step - loss: 0.6787 - accuracy: 0.5138 - val_loss: 0.6783 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00353: val_accuracy did not improve from 0.83000\n",
      "Epoch 354/2000\n",
      "800/800 [==============================] - 9s 12ms/step - loss: 0.6791 - accuracy: 0.5188 - val_loss: 0.6776 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00354: val_accuracy did not improve from 0.83000\n",
      "Epoch 355/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6787 - accuracy: 0.5375 - val_loss: 0.6763 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00355: val_accuracy did not improve from 0.83000\n",
      "Epoch 356/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6779 - accuracy: 0.5150 - val_loss: 0.6767 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00356: val_accuracy did not improve from 0.83000\n",
      "Epoch 357/2000\n",
      "800/800 [==============================] - 8s 11ms/step - loss: 0.6775 - accuracy: 0.5275 - val_loss: 0.6760 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00357: val_accuracy did not improve from 0.83000\n",
      "Epoch 358/2000\n",
      "800/800 [==============================] - 9s 12ms/step - loss: 0.6786 - accuracy: 0.5000 - val_loss: 0.6758 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00358: val_accuracy did not improve from 0.83000\n",
      "Epoch 359/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6803 - accuracy: 0.4938 - val_loss: 0.6771 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00359: val_accuracy did not improve from 0.83000\n",
      "Epoch 360/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 13s 16ms/step - loss: 0.6800 - accuracy: 0.4888 - val_loss: 0.6759 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00360: val_accuracy did not improve from 0.83000\n",
      "Epoch 361/2000\n",
      "800/800 [==============================] - 10s 12ms/step - loss: 0.6800 - accuracy: 0.5000 - val_loss: 0.6765 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00361: val_accuracy did not improve from 0.83000\n",
      "Epoch 362/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6794 - accuracy: 0.4988 - val_loss: 0.6767 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00362: val_accuracy did not improve from 0.83000\n",
      "Epoch 363/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6783 - accuracy: 0.5063 - val_loss: 0.6766 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00363: val_accuracy did not improve from 0.83000\n",
      "Epoch 364/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6792 - accuracy: 0.5213 - val_loss: 0.6760 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00364: val_accuracy did not improve from 0.83000\n",
      "Epoch 365/2000\n",
      "800/800 [==============================] - 10s 12ms/step - loss: 0.6787 - accuracy: 0.5125 - val_loss: 0.6753 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00365: val_accuracy did not improve from 0.83000\n",
      "Epoch 366/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6794 - accuracy: 0.5013 - val_loss: 0.6771 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00366: val_accuracy did not improve from 0.83000\n",
      "Epoch 367/2000\n",
      "800/800 [==============================] - 8s 9ms/step - loss: 0.6791 - accuracy: 0.4888 - val_loss: 0.6767 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00367: val_accuracy did not improve from 0.83000\n",
      "Epoch 368/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6794 - accuracy: 0.5088 - val_loss: 0.6761 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00368: val_accuracy did not improve from 0.83000\n",
      "Epoch 369/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6781 - accuracy: 0.5213 - val_loss: 0.6745 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00369: val_accuracy did not improve from 0.83000\n",
      "Epoch 370/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6794 - accuracy: 0.5163 - val_loss: 0.6751 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00370: val_accuracy did not improve from 0.83000\n",
      "Epoch 371/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6798 - accuracy: 0.5100 - val_loss: 0.6753 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00371: val_accuracy did not improve from 0.83000\n",
      "Epoch 372/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6768 - accuracy: 0.5250 - val_loss: 0.6763 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00372: val_accuracy did not improve from 0.83000\n",
      "Epoch 373/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6787 - accuracy: 0.5275 - val_loss: 0.6751 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00373: val_accuracy did not improve from 0.83000\n",
      "Epoch 374/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6789 - accuracy: 0.5362 - val_loss: 0.6762 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00374: val_accuracy did not improve from 0.83000\n",
      "Epoch 375/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6792 - accuracy: 0.4950 - val_loss: 0.6762 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00375: val_accuracy did not improve from 0.83000\n",
      "Epoch 376/2000\n",
      "800/800 [==============================] - 10s 13ms/step - loss: 0.6777 - accuracy: 0.5250 - val_loss: 0.6759 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00376: val_accuracy did not improve from 0.83000\n",
      "Epoch 377/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6800 - accuracy: 0.5013 - val_loss: 0.6765 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00377: val_accuracy did not improve from 0.83000\n",
      "Epoch 378/2000\n",
      "800/800 [==============================] - 8s 9ms/step - loss: 0.6794 - accuracy: 0.5063 - val_loss: 0.6794 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00378: val_accuracy did not improve from 0.83000\n",
      "Epoch 379/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6794 - accuracy: 0.5175 - val_loss: 0.6796 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00379: val_accuracy did not improve from 0.83000\n",
      "Epoch 380/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6784 - accuracy: 0.5163 - val_loss: 0.6778 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00380: val_accuracy did not improve from 0.83000\n",
      "Epoch 381/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6791 - accuracy: 0.4938 - val_loss: 0.6773 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00381: val_accuracy did not improve from 0.83000\n",
      "Epoch 382/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6792 - accuracy: 0.4963 - val_loss: 0.6779 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00382: val_accuracy did not improve from 0.83000\n",
      "Epoch 383/2000\n",
      "800/800 [==============================] - 9s 12ms/step - loss: 0.6783 - accuracy: 0.5412 - val_loss: 0.6775 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00383: val_accuracy did not improve from 0.83000\n",
      "Epoch 384/2000\n",
      "800/800 [==============================] - 10s 13ms/step - loss: 0.6792 - accuracy: 0.5088 - val_loss: 0.6773 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00384: val_accuracy did not improve from 0.83000\n",
      "Epoch 385/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6777 - accuracy: 0.5188 - val_loss: 0.6764 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00385: val_accuracy did not improve from 0.83000\n",
      "Epoch 386/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6788 - accuracy: 0.5163 - val_loss: 0.6761 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00386: val_accuracy did not improve from 0.83000\n",
      "Epoch 387/2000\n",
      "800/800 [==============================] - 8s 11ms/step - loss: 0.6804 - accuracy: 0.4725 - val_loss: 0.6761 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00387: val_accuracy did not improve from 0.83000\n",
      "Epoch 388/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6792 - accuracy: 0.4675 - val_loss: 0.6777 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00388: val_accuracy did not improve from 0.83000\n",
      "Epoch 389/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6780 - accuracy: 0.4988 - val_loss: 0.6780 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00389: val_accuracy did not improve from 0.83000\n",
      "Epoch 390/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6803 - accuracy: 0.4988 - val_loss: 0.6774 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00390: val_accuracy did not improve from 0.83000\n",
      "Epoch 391/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6788 - accuracy: 0.5163 - val_loss: 0.6772 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00391: val_accuracy did not improve from 0.83000\n",
      "Epoch 392/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6781 - accuracy: 0.5100 - val_loss: 0.6771 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00392: val_accuracy did not improve from 0.83000\n",
      "Epoch 393/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6776 - accuracy: 0.4963 - val_loss: 0.6767 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00393: val_accuracy did not improve from 0.83000\n",
      "Epoch 394/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6777 - accuracy: 0.5400 - val_loss: 0.6766 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00394: val_accuracy did not improve from 0.83000\n",
      "Epoch 395/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6795 - accuracy: 0.5100 - val_loss: 0.6768 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00395: val_accuracy did not improve from 0.83000\n",
      "Epoch 396/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6801 - accuracy: 0.5013 - val_loss: 0.6771 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00396: val_accuracy did not improve from 0.83000\n",
      "Epoch 397/2000\n",
      "800/800 [==============================] - 8s 9ms/step - loss: 0.6789 - accuracy: 0.4988 - val_loss: 0.6759 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00397: val_accuracy did not improve from 0.83000\n",
      "Epoch 398/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6794 - accuracy: 0.5075 - val_loss: 0.6764 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00398: val_accuracy did not improve from 0.83000\n",
      "Epoch 399/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6791 - accuracy: 0.5063 - val_loss: 0.6775 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00399: val_accuracy did not improve from 0.83000\n",
      "Epoch 400/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6776 - accuracy: 0.5075 - val_loss: 0.6781 - val_accuracy: 0.4550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00400: val_accuracy did not improve from 0.83000\n",
      "Epoch 401/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6786 - accuracy: 0.5412 - val_loss: 0.6762 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00401: val_accuracy did not improve from 0.83000\n",
      "Epoch 402/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6802 - accuracy: 0.5025 - val_loss: 0.6766 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00402: val_accuracy did not improve from 0.83000\n",
      "Epoch 403/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6784 - accuracy: 0.5013 - val_loss: 0.6763 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00403: val_accuracy did not improve from 0.83000\n",
      "Epoch 404/2000\n",
      "800/800 [==============================] - 11s 13ms/step - loss: 0.6789 - accuracy: 0.4875 - val_loss: 0.6779 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00404: val_accuracy did not improve from 0.83000\n",
      "Epoch 405/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6792 - accuracy: 0.5038 - val_loss: 0.6787 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00405: val_accuracy did not improve from 0.83000\n",
      "Epoch 406/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6808 - accuracy: 0.5013 - val_loss: 0.6783 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00406: val_accuracy did not improve from 0.83000\n",
      "Epoch 407/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6788 - accuracy: 0.5113 - val_loss: 0.6765 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00407: val_accuracy did not improve from 0.83000\n",
      "Epoch 408/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6788 - accuracy: 0.5000 - val_loss: 0.6760 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00408: val_accuracy did not improve from 0.83000\n",
      "Epoch 409/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6790 - accuracy: 0.5362 - val_loss: 0.6755 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00409: val_accuracy did not improve from 0.83000\n",
      "Epoch 410/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6810 - accuracy: 0.4613 - val_loss: 0.6765 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00410: val_accuracy did not improve from 0.83000\n",
      "Epoch 411/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6795 - accuracy: 0.4863 - val_loss: 0.6768 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00411: val_accuracy did not improve from 0.83000\n",
      "Epoch 412/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6781 - accuracy: 0.5312 - val_loss: 0.6769 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00412: val_accuracy did not improve from 0.83000\n",
      "Epoch 413/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6783 - accuracy: 0.5387 - val_loss: 0.6765 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00413: val_accuracy did not improve from 0.83000\n",
      "Epoch 414/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6777 - accuracy: 0.5038 - val_loss: 0.6761 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00414: val_accuracy did not improve from 0.83000\n",
      "Epoch 415/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6786 - accuracy: 0.5387 - val_loss: 0.6766 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00415: val_accuracy did not improve from 0.83000\n",
      "Epoch 416/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6790 - accuracy: 0.5088 - val_loss: 0.6759 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00416: val_accuracy did not improve from 0.83000\n",
      "Epoch 417/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6793 - accuracy: 0.4812 - val_loss: 0.6761 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00417: val_accuracy did not improve from 0.83000\n",
      "Epoch 418/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6787 - accuracy: 0.5163 - val_loss: 0.6763 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00418: val_accuracy did not improve from 0.83000\n",
      "Epoch 419/2000\n",
      "800/800 [==============================] - 9s 12ms/step - loss: 0.6781 - accuracy: 0.5200 - val_loss: 0.6759 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00419: val_accuracy did not improve from 0.83000\n",
      "Epoch 420/2000\n",
      "800/800 [==============================] - 9s 12ms/step - loss: 0.6791 - accuracy: 0.5200 - val_loss: 0.6754 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00420: val_accuracy did not improve from 0.83000\n",
      "Epoch 421/2000\n",
      "800/800 [==============================] - 10s 12ms/step - loss: 0.6802 - accuracy: 0.4975 - val_loss: 0.6765 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00421: val_accuracy did not improve from 0.83000\n",
      "Epoch 422/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6784 - accuracy: 0.5088 - val_loss: 0.6764 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00422: val_accuracy did not improve from 0.83000\n",
      "Epoch 423/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6791 - accuracy: 0.5238 - val_loss: 0.6776 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00423: val_accuracy did not improve from 0.83000\n",
      "Epoch 424/2000\n",
      "800/800 [==============================] - 8s 9ms/step - loss: 0.6801 - accuracy: 0.4850 - val_loss: 0.6761 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00424: val_accuracy did not improve from 0.83000\n",
      "Epoch 425/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6805 - accuracy: 0.4950 - val_loss: 0.6754 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00425: val_accuracy did not improve from 0.83000\n",
      "Epoch 426/2000\n",
      "800/800 [==============================] - 9s 12ms/step - loss: 0.6788 - accuracy: 0.5013 - val_loss: 0.6767 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00426: val_accuracy did not improve from 0.83000\n",
      "Epoch 427/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6792 - accuracy: 0.5275 - val_loss: 0.6766 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00427: val_accuracy did not improve from 0.83000\n",
      "Epoch 428/2000\n",
      "800/800 [==============================] - 10s 12ms/step - loss: 0.6794 - accuracy: 0.5038 - val_loss: 0.6767 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00428: val_accuracy did not improve from 0.83000\n",
      "Epoch 429/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6786 - accuracy: 0.5238 - val_loss: 0.6774 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00429: val_accuracy did not improve from 0.83000\n",
      "Epoch 430/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6786 - accuracy: 0.4963 - val_loss: 0.6772 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00430: val_accuracy did not improve from 0.83000\n",
      "Epoch 431/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6789 - accuracy: 0.5113 - val_loss: 0.6761 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00431: val_accuracy did not improve from 0.83000\n",
      "Epoch 432/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6794 - accuracy: 0.4875 - val_loss: 0.6761 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00432: val_accuracy did not improve from 0.83000\n",
      "Epoch 433/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6778 - accuracy: 0.5387 - val_loss: 0.6756 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00433: val_accuracy did not improve from 0.83000\n",
      "Epoch 434/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6782 - accuracy: 0.5000 - val_loss: 0.6759 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00434: val_accuracy did not improve from 0.83000\n",
      "Epoch 435/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6796 - accuracy: 0.5312 - val_loss: 0.6762 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00435: val_accuracy did not improve from 0.83000\n",
      "Epoch 436/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6797 - accuracy: 0.5038 - val_loss: 0.6761 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00436: val_accuracy did not improve from 0.83000\n",
      "Epoch 437/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6783 - accuracy: 0.5275 - val_loss: 0.6767 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00437: val_accuracy did not improve from 0.83000\n",
      "Epoch 438/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6775 - accuracy: 0.5437 - val_loss: 0.6774 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00438: val_accuracy did not improve from 0.83000\n",
      "Epoch 439/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6797 - accuracy: 0.5025 - val_loss: 0.6767 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00439: val_accuracy did not improve from 0.83000\n",
      "Epoch 440/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6789 - accuracy: 0.5150 - val_loss: 0.6780 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00440: val_accuracy did not improve from 0.83000\n",
      "Epoch 441/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6795 - accuracy: 0.5000 - val_loss: 0.6770 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00441: val_accuracy did not improve from 0.83000\n",
      "Epoch 442/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6791 - accuracy: 0.5038 - val_loss: 0.6769 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00442: val_accuracy did not improve from 0.83000\n",
      "Epoch 443/2000\n",
      "800/800 [==============================] - 8s 9ms/step - loss: 0.6802 - accuracy: 0.4863 - val_loss: 0.6770 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00443: val_accuracy did not improve from 0.83000\n",
      "Epoch 444/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6770 - accuracy: 0.4825 - val_loss: 0.6766 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00444: val_accuracy did not improve from 0.83000\n",
      "Epoch 445/2000\n",
      "800/800 [==============================] - 9s 12ms/step - loss: 0.6790 - accuracy: 0.4850 - val_loss: 0.6758 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00445: val_accuracy did not improve from 0.83000\n",
      "Epoch 446/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6787 - accuracy: 0.5188 - val_loss: 0.6757 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00446: val_accuracy did not improve from 0.83000\n",
      "Epoch 447/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6788 - accuracy: 0.5163 - val_loss: 0.6758 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00447: val_accuracy did not improve from 0.83000\n",
      "Epoch 448/2000\n",
      "800/800 [==============================] - 8s 9ms/step - loss: 0.6793 - accuracy: 0.5025 - val_loss: 0.6763 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00448: val_accuracy did not improve from 0.83000\n",
      "Epoch 449/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6798 - accuracy: 0.5088 - val_loss: 0.6773 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00449: val_accuracy did not improve from 0.83000\n",
      "Epoch 450/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6790 - accuracy: 0.5038 - val_loss: 0.6786 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00450: val_accuracy did not improve from 0.83000\n",
      "Epoch 451/2000\n",
      "800/800 [==============================] - 8s 9ms/step - loss: 0.6794 - accuracy: 0.5100 - val_loss: 0.6774 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00451: val_accuracy did not improve from 0.83000\n",
      "Epoch 452/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6780 - accuracy: 0.5325 - val_loss: 0.6767 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00452: val_accuracy did not improve from 0.83000\n",
      "Epoch 453/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6798 - accuracy: 0.4938 - val_loss: 0.6767 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00453: val_accuracy did not improve from 0.83000\n",
      "Epoch 454/2000\n",
      "800/800 [==============================] - 8s 9ms/step - loss: 0.6781 - accuracy: 0.5238 - val_loss: 0.6774 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00454: val_accuracy did not improve from 0.83000\n",
      "Epoch 455/2000\n",
      "800/800 [==============================] - 12s 15ms/step - loss: 0.6773 - accuracy: 0.5225 - val_loss: 0.6785 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00455: val_accuracy did not improve from 0.83000\n",
      "Epoch 456/2000\n",
      "800/800 [==============================] - 12s 15ms/step - loss: 0.6788 - accuracy: 0.5038 - val_loss: 0.6780 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00456: val_accuracy did not improve from 0.83000\n",
      "Epoch 457/2000\n",
      "800/800 [==============================] - 10s 12ms/step - loss: 0.6801 - accuracy: 0.5000 - val_loss: 0.6763 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00457: val_accuracy did not improve from 0.83000\n",
      "Epoch 458/2000\n",
      "800/800 [==============================] - 10s 12ms/step - loss: 0.6787 - accuracy: 0.5337 - val_loss: 0.6776 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00458: val_accuracy did not improve from 0.83000\n",
      "Epoch 459/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6787 - accuracy: 0.5125 - val_loss: 0.6764 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00459: val_accuracy did not improve from 0.83000\n",
      "Epoch 460/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6784 - accuracy: 0.4950 - val_loss: 0.6756 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00460: val_accuracy did not improve from 0.83000\n",
      "Epoch 461/2000\n",
      "800/800 [==============================] - 9s 12ms/step - loss: 0.6783 - accuracy: 0.5412 - val_loss: 0.6754 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00461: val_accuracy did not improve from 0.83000\n",
      "Epoch 462/2000\n",
      "800/800 [==============================] - 8s 11ms/step - loss: 0.6796 - accuracy: 0.4975 - val_loss: 0.6764 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00462: val_accuracy did not improve from 0.83000\n",
      "Epoch 463/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6799 - accuracy: 0.5050 - val_loss: 0.6761 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00463: val_accuracy did not improve from 0.83000\n",
      "Epoch 464/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6779 - accuracy: 0.5163 - val_loss: 0.6783 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00464: val_accuracy did not improve from 0.83000\n",
      "Epoch 465/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6777 - accuracy: 0.5300 - val_loss: 0.6785 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00465: val_accuracy did not improve from 0.83000\n",
      "Epoch 466/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6782 - accuracy: 0.5050 - val_loss: 0.6784 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00466: val_accuracy did not improve from 0.83000\n",
      "Epoch 467/2000\n",
      "800/800 [==============================] - 11s 13ms/step - loss: 0.6802 - accuracy: 0.4950 - val_loss: 0.6782 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00467: val_accuracy did not improve from 0.83000\n",
      "Epoch 468/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6790 - accuracy: 0.5013 - val_loss: 0.6776 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00468: val_accuracy did not improve from 0.83000\n",
      "Epoch 469/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6790 - accuracy: 0.5113 - val_loss: 0.6770 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00469: val_accuracy did not improve from 0.83000\n",
      "Epoch 470/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6787 - accuracy: 0.4913 - val_loss: 0.6767 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00470: val_accuracy did not improve from 0.83000\n",
      "Epoch 471/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6788 - accuracy: 0.4988 - val_loss: 0.6767 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00471: val_accuracy did not improve from 0.83000\n",
      "Epoch 472/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6784 - accuracy: 0.5250 - val_loss: 0.6763 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00472: val_accuracy did not improve from 0.83000\n",
      "Epoch 473/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6796 - accuracy: 0.4925 - val_loss: 0.6762 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00473: val_accuracy did not improve from 0.83000\n",
      "Epoch 474/2000\n",
      "800/800 [==============================] - 10s 12ms/step - loss: 0.6793 - accuracy: 0.5000 - val_loss: 0.6761 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00474: val_accuracy did not improve from 0.83000\n",
      "Epoch 475/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6797 - accuracy: 0.4975 - val_loss: 0.6761 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00475: val_accuracy did not improve from 0.83000\n",
      "Epoch 476/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6781 - accuracy: 0.5125 - val_loss: 0.6764 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00476: val_accuracy did not improve from 0.83000\n",
      "Epoch 477/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6777 - accuracy: 0.5275 - val_loss: 0.6767 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00477: val_accuracy did not improve from 0.83000\n",
      "Epoch 478/2000\n",
      "800/800 [==============================] - 10s 12ms/step - loss: 0.6789 - accuracy: 0.5300 - val_loss: 0.6776 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00478: val_accuracy did not improve from 0.83000\n",
      "Epoch 479/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6797 - accuracy: 0.5000 - val_loss: 0.6778 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00479: val_accuracy did not improve from 0.83000\n",
      "Epoch 480/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6782 - accuracy: 0.5175 - val_loss: 0.6779 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00480: val_accuracy did not improve from 0.83000\n",
      "Epoch 481/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6782 - accuracy: 0.5063 - val_loss: 0.6777 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00481: val_accuracy did not improve from 0.83000\n",
      "Epoch 482/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6791 - accuracy: 0.5150 - val_loss: 0.6765 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00482: val_accuracy did not improve from 0.83000\n",
      "Epoch 483/2000\n",
      "800/800 [==============================] - 10s 12ms/step - loss: 0.6787 - accuracy: 0.5050 - val_loss: 0.6764 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00483: val_accuracy did not improve from 0.83000\n",
      "Epoch 484/2000\n",
      "800/800 [==============================] - 9s 12ms/step - loss: 0.6795 - accuracy: 0.5113 - val_loss: 0.6776 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00484: val_accuracy did not improve from 0.83000\n",
      "Epoch 485/2000\n",
      "800/800 [==============================] - 9s 12ms/step - loss: 0.6789 - accuracy: 0.4950 - val_loss: 0.6768 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00485: val_accuracy did not improve from 0.83000\n",
      "Epoch 486/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6785 - accuracy: 0.5213 - val_loss: 0.6764 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00486: val_accuracy did not improve from 0.83000\n",
      "Epoch 487/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6784 - accuracy: 0.5288 - val_loss: 0.6771 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00487: val_accuracy did not improve from 0.83000\n",
      "Epoch 488/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6792 - accuracy: 0.5013 - val_loss: 0.6762 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00488: val_accuracy did not improve from 0.83000\n",
      "Epoch 489/2000\n",
      "800/800 [==============================] - 10s 12ms/step - loss: 0.6789 - accuracy: 0.5225 - val_loss: 0.6772 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00489: val_accuracy did not improve from 0.83000\n",
      "Epoch 490/2000\n",
      "800/800 [==============================] - 10s 13ms/step - loss: 0.6792 - accuracy: 0.5000 - val_loss: 0.6756 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00490: val_accuracy did not improve from 0.83000\n",
      "Epoch 491/2000\n",
      "800/800 [==============================] - 11s 13ms/step - loss: 0.6788 - accuracy: 0.5213 - val_loss: 0.6750 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00491: val_accuracy did not improve from 0.83000\n",
      "Epoch 492/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6782 - accuracy: 0.5175 - val_loss: 0.6760 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00492: val_accuracy did not improve from 0.83000\n",
      "Epoch 493/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6788 - accuracy: 0.5412 - val_loss: 0.6754 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00493: val_accuracy did not improve from 0.83000\n",
      "Epoch 494/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6782 - accuracy: 0.5188 - val_loss: 0.6774 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00494: val_accuracy did not improve from 0.83000\n",
      "Epoch 495/2000\n",
      "800/800 [==============================] - 8s 11ms/step - loss: 0.6782 - accuracy: 0.5462 - val_loss: 0.6777 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00495: val_accuracy did not improve from 0.83000\n",
      "Epoch 496/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6798 - accuracy: 0.4950 - val_loss: 0.6775 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00496: val_accuracy did not improve from 0.83000\n",
      "Epoch 497/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6778 - accuracy: 0.5213 - val_loss: 0.6769 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00497: val_accuracy did not improve from 0.83000\n",
      "Epoch 498/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6799 - accuracy: 0.4975 - val_loss: 0.6781 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00498: val_accuracy did not improve from 0.83000\n",
      "Epoch 499/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6788 - accuracy: 0.5000 - val_loss: 0.6763 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00499: val_accuracy did not improve from 0.83000\n",
      "Epoch 500/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6800 - accuracy: 0.4988 - val_loss: 0.6772 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00500: val_accuracy did not improve from 0.83000\n",
      "Epoch 501/2000\n",
      "800/800 [==============================] - 9s 12ms/step - loss: 0.6798 - accuracy: 0.4863 - val_loss: 0.6771 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00501: val_accuracy did not improve from 0.83000\n",
      "Epoch 502/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6793 - accuracy: 0.5213 - val_loss: 0.6764 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00502: val_accuracy did not improve from 0.83000\n",
      "Epoch 503/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6782 - accuracy: 0.4888 - val_loss: 0.6763 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00503: val_accuracy did not improve from 0.83000\n",
      "Epoch 504/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6781 - accuracy: 0.5000 - val_loss: 0.6777 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00504: val_accuracy did not improve from 0.83000\n",
      "Epoch 505/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6790 - accuracy: 0.4963 - val_loss: 0.6768 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00505: val_accuracy did not improve from 0.83000\n",
      "Epoch 506/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6795 - accuracy: 0.4975 - val_loss: 0.6760 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00506: val_accuracy did not improve from 0.83000\n",
      "Epoch 507/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6793 - accuracy: 0.4963 - val_loss: 0.6773 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00507: val_accuracy did not improve from 0.83000\n",
      "Epoch 508/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6791 - accuracy: 0.5225 - val_loss: 0.6767 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00508: val_accuracy did not improve from 0.83000\n",
      "Epoch 509/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6799 - accuracy: 0.5038 - val_loss: 0.6771 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00509: val_accuracy did not improve from 0.83000\n",
      "Epoch 510/2000\n",
      "800/800 [==============================] - 11s 13ms/step - loss: 0.6785 - accuracy: 0.5300 - val_loss: 0.6769 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00510: val_accuracy did not improve from 0.83000\n",
      "Epoch 511/2000\n",
      "800/800 [==============================] - 10s 13ms/step - loss: 0.6804 - accuracy: 0.5050 - val_loss: 0.6765 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00511: val_accuracy did not improve from 0.83000\n",
      "Epoch 512/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6792 - accuracy: 0.5063 - val_loss: 0.6765 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00512: val_accuracy did not improve from 0.83000\n",
      "Epoch 513/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6797 - accuracy: 0.4588 - val_loss: 0.6771 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00513: val_accuracy did not improve from 0.83000\n",
      "Epoch 514/2000\n",
      "800/800 [==============================] - 8s 9ms/step - loss: 0.6787 - accuracy: 0.5113 - val_loss: 0.6763 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00514: val_accuracy did not improve from 0.83000\n",
      "Epoch 515/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6786 - accuracy: 0.4950 - val_loss: 0.6773 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00515: val_accuracy did not improve from 0.83000\n",
      "Epoch 516/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6797 - accuracy: 0.5200 - val_loss: 0.6766 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00516: val_accuracy did not improve from 0.83000\n",
      "Epoch 517/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6799 - accuracy: 0.4825 - val_loss: 0.6767 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00517: val_accuracy did not improve from 0.83000\n",
      "Epoch 518/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6790 - accuracy: 0.5275 - val_loss: 0.6766 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00518: val_accuracy did not improve from 0.83000\n",
      "Epoch 519/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6788 - accuracy: 0.4925 - val_loss: 0.6767 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00519: val_accuracy did not improve from 0.83000\n",
      "Epoch 520/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6786 - accuracy: 0.5288 - val_loss: 0.6773 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00520: val_accuracy did not improve from 0.83000\n",
      "Epoch 521/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6797 - accuracy: 0.4963 - val_loss: 0.6772 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00521: val_accuracy did not improve from 0.83000\n",
      "Epoch 522/2000\n",
      "800/800 [==============================] - 8s 9ms/step - loss: 0.6795 - accuracy: 0.4825 - val_loss: 0.6763 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00522: val_accuracy did not improve from 0.83000\n",
      "Epoch 523/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6787 - accuracy: 0.5225 - val_loss: 0.6773 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00523: val_accuracy did not improve from 0.83000\n",
      "Epoch 524/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6786 - accuracy: 0.5263 - val_loss: 0.6784 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00524: val_accuracy did not improve from 0.83000\n",
      "Epoch 525/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6784 - accuracy: 0.5238 - val_loss: 0.6782 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00525: val_accuracy did not improve from 0.83000\n",
      "Epoch 526/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6791 - accuracy: 0.5025 - val_loss: 0.6781 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00526: val_accuracy did not improve from 0.83000\n",
      "Epoch 527/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6793 - accuracy: 0.5188 - val_loss: 0.6772 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00527: val_accuracy did not improve from 0.83000\n",
      "Epoch 528/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6781 - accuracy: 0.5500 - val_loss: 0.6764 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00528: val_accuracy did not improve from 0.83000\n",
      "Epoch 529/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6794 - accuracy: 0.4925 - val_loss: 0.6752 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00529: val_accuracy did not improve from 0.83000\n",
      "Epoch 530/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6789 - accuracy: 0.5075 - val_loss: 0.6754 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00530: val_accuracy did not improve from 0.83000\n",
      "Epoch 531/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6782 - accuracy: 0.5288 - val_loss: 0.6760 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00531: val_accuracy did not improve from 0.83000\n",
      "Epoch 532/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6781 - accuracy: 0.5100 - val_loss: 0.6753 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00532: val_accuracy did not improve from 0.83000\n",
      "Epoch 533/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6792 - accuracy: 0.4950 - val_loss: 0.6755 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00533: val_accuracy did not improve from 0.83000\n",
      "Epoch 534/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6787 - accuracy: 0.5063 - val_loss: 0.6758 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00534: val_accuracy did not improve from 0.83000\n",
      "Epoch 535/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6767 - accuracy: 0.5325 - val_loss: 0.6765 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00535: val_accuracy did not improve from 0.83000\n",
      "Epoch 536/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6785 - accuracy: 0.5163 - val_loss: 0.6768 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00536: val_accuracy did not improve from 0.83000\n",
      "Epoch 537/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6780 - accuracy: 0.5325 - val_loss: 0.6776 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00537: val_accuracy did not improve from 0.83000\n",
      "Epoch 538/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6797 - accuracy: 0.5088 - val_loss: 0.6770 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00538: val_accuracy did not improve from 0.83000\n",
      "Epoch 539/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6784 - accuracy: 0.4938 - val_loss: 0.6764 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00539: val_accuracy did not improve from 0.83000\n",
      "Epoch 540/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6777 - accuracy: 0.5325 - val_loss: 0.6767 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00540: val_accuracy did not improve from 0.83000\n",
      "Epoch 541/2000\n",
      "800/800 [==============================] - 9s 12ms/step - loss: 0.6788 - accuracy: 0.5263 - val_loss: 0.6767 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00541: val_accuracy did not improve from 0.83000\n",
      "Epoch 542/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6777 - accuracy: 0.5300 - val_loss: 0.6766 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00542: val_accuracy did not improve from 0.83000\n",
      "Epoch 543/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6791 - accuracy: 0.5025 - val_loss: 0.6765 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00543: val_accuracy did not improve from 0.83000\n",
      "Epoch 544/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6787 - accuracy: 0.5075 - val_loss: 0.6771 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00544: val_accuracy did not improve from 0.83000\n",
      "Epoch 545/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6783 - accuracy: 0.5200 - val_loss: 0.6757 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00545: val_accuracy did not improve from 0.83000\n",
      "Epoch 546/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6779 - accuracy: 0.5200 - val_loss: 0.6758 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00546: val_accuracy did not improve from 0.83000\n",
      "Epoch 547/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6797 - accuracy: 0.5337 - val_loss: 0.6762 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00547: val_accuracy did not improve from 0.83000\n",
      "Epoch 548/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6799 - accuracy: 0.4775 - val_loss: 0.6763 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00548: val_accuracy did not improve from 0.83000\n",
      "Epoch 549/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6796 - accuracy: 0.5150 - val_loss: 0.6777 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00549: val_accuracy did not improve from 0.83000\n",
      "Epoch 550/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6779 - accuracy: 0.5175 - val_loss: 0.6770 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00550: val_accuracy did not improve from 0.83000\n",
      "Epoch 551/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6791 - accuracy: 0.5350 - val_loss: 0.6770 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00551: val_accuracy did not improve from 0.83000\n",
      "Epoch 552/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6783 - accuracy: 0.5312 - val_loss: 0.6769 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00552: val_accuracy did not improve from 0.83000\n",
      "Epoch 553/2000\n",
      "800/800 [==============================] - 8s 9ms/step - loss: 0.6765 - accuracy: 0.5450 - val_loss: 0.6772 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00553: val_accuracy did not improve from 0.83000\n",
      "Epoch 554/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6778 - accuracy: 0.5038 - val_loss: 0.6768 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00554: val_accuracy did not improve from 0.83000\n",
      "Epoch 555/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6785 - accuracy: 0.5000 - val_loss: 0.6763 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00555: val_accuracy did not improve from 0.83000\n",
      "Epoch 556/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6790 - accuracy: 0.5138 - val_loss: 0.6767 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00556: val_accuracy did not improve from 0.83000\n",
      "Epoch 557/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6797 - accuracy: 0.5025 - val_loss: 0.6770 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00557: val_accuracy did not improve from 0.83000\n",
      "Epoch 558/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6798 - accuracy: 0.5138 - val_loss: 0.6767 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00558: val_accuracy did not improve from 0.83000\n",
      "Epoch 559/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6792 - accuracy: 0.5263 - val_loss: 0.6766 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00559: val_accuracy did not improve from 0.83000\n",
      "Epoch 560/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6797 - accuracy: 0.5113 - val_loss: 0.6764 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00560: val_accuracy did not improve from 0.83000\n",
      "Epoch 561/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6795 - accuracy: 0.4625 - val_loss: 0.6772 - val_accuracy: 0.4550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00561: val_accuracy did not improve from 0.83000\n",
      "Epoch 562/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6785 - accuracy: 0.4863 - val_loss: 0.6762 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00562: val_accuracy did not improve from 0.83000\n",
      "Epoch 563/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6787 - accuracy: 0.4963 - val_loss: 0.6764 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00563: val_accuracy did not improve from 0.83000\n",
      "Epoch 564/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6795 - accuracy: 0.5088 - val_loss: 0.6758 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00564: val_accuracy did not improve from 0.83000\n",
      "Epoch 565/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6776 - accuracy: 0.5150 - val_loss: 0.6759 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00565: val_accuracy did not improve from 0.83000\n",
      "Epoch 566/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6790 - accuracy: 0.5275 - val_loss: 0.6763 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00566: val_accuracy did not improve from 0.83000\n",
      "Epoch 567/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6779 - accuracy: 0.5075 - val_loss: 0.6765 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00567: val_accuracy did not improve from 0.83000\n",
      "Epoch 568/2000\n",
      "800/800 [==============================] - 8s 9ms/step - loss: 0.6776 - accuracy: 0.5013 - val_loss: 0.6774 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00568: val_accuracy did not improve from 0.83000\n",
      "Epoch 569/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6804 - accuracy: 0.4988 - val_loss: 0.6770 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00569: val_accuracy did not improve from 0.83000\n",
      "Epoch 570/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6765 - accuracy: 0.5400 - val_loss: 0.6772 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00570: val_accuracy did not improve from 0.83000\n",
      "Epoch 571/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6789 - accuracy: 0.5063 - val_loss: 0.6771 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00571: val_accuracy did not improve from 0.83000\n",
      "Epoch 572/2000\n",
      "800/800 [==============================] - 8s 9ms/step - loss: 0.6786 - accuracy: 0.5113 - val_loss: 0.6760 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00572: val_accuracy did not improve from 0.83000\n",
      "Epoch 573/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6796 - accuracy: 0.5088 - val_loss: 0.6755 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00573: val_accuracy did not improve from 0.83000\n",
      "Epoch 574/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6781 - accuracy: 0.5213 - val_loss: 0.6755 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00574: val_accuracy did not improve from 0.83000\n",
      "Epoch 575/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6780 - accuracy: 0.5200 - val_loss: 0.6754 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00575: val_accuracy did not improve from 0.83000\n",
      "Epoch 576/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6790 - accuracy: 0.5288 - val_loss: 0.6758 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00576: val_accuracy did not improve from 0.83000\n",
      "Epoch 577/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6778 - accuracy: 0.5325 - val_loss: 0.6759 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00577: val_accuracy did not improve from 0.83000\n",
      "Epoch 578/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6800 - accuracy: 0.4863 - val_loss: 0.6770 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00578: val_accuracy did not improve from 0.83000\n",
      "Epoch 579/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6791 - accuracy: 0.5100 - val_loss: 0.6773 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00579: val_accuracy did not improve from 0.83000\n",
      "Epoch 580/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6799 - accuracy: 0.4975 - val_loss: 0.6774 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00580: val_accuracy did not improve from 0.83000\n",
      "Epoch 581/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6789 - accuracy: 0.5050 - val_loss: 0.6764 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00581: val_accuracy did not improve from 0.83000\n",
      "Epoch 582/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6784 - accuracy: 0.5225 - val_loss: 0.6765 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00582: val_accuracy did not improve from 0.83000\n",
      "Epoch 583/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6783 - accuracy: 0.4863 - val_loss: 0.6773 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00583: val_accuracy did not improve from 0.83000\n",
      "Epoch 584/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6791 - accuracy: 0.5300 - val_loss: 0.6769 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00584: val_accuracy did not improve from 0.83000\n",
      "Epoch 585/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6788 - accuracy: 0.5100 - val_loss: 0.6768 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00585: val_accuracy did not improve from 0.83000\n",
      "Epoch 586/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6782 - accuracy: 0.5113 - val_loss: 0.6776 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00586: val_accuracy did not improve from 0.83000\n",
      "Epoch 587/2000\n",
      "800/800 [==============================] - 8s 9ms/step - loss: 0.6787 - accuracy: 0.5038 - val_loss: 0.6773 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00587: val_accuracy did not improve from 0.83000\n",
      "Epoch 588/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6794 - accuracy: 0.4975 - val_loss: 0.6772 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00588: val_accuracy did not improve from 0.83000\n",
      "Epoch 589/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6778 - accuracy: 0.5188 - val_loss: 0.6768 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00589: val_accuracy did not improve from 0.83000\n",
      "Epoch 590/2000\n",
      "800/800 [==============================] - 8s 9ms/step - loss: 0.6790 - accuracy: 0.5088 - val_loss: 0.6768 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00590: val_accuracy did not improve from 0.83000\n",
      "Epoch 591/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6784 - accuracy: 0.5225 - val_loss: 0.6763 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00591: val_accuracy did not improve from 0.83000\n",
      "Epoch 592/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6787 - accuracy: 0.5500 - val_loss: 0.6761 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00592: val_accuracy did not improve from 0.83000\n",
      "Epoch 593/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6788 - accuracy: 0.5213 - val_loss: 0.6758 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00593: val_accuracy did not improve from 0.83000\n",
      "Epoch 594/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6763 - accuracy: 0.5500 - val_loss: 0.6767 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00594: val_accuracy did not improve from 0.83000\n",
      "Epoch 595/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6782 - accuracy: 0.4975 - val_loss: 0.6761 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00595: val_accuracy did not improve from 0.83000\n",
      "Epoch 596/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6766 - accuracy: 0.5387 - val_loss: 0.6764 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00596: val_accuracy did not improve from 0.83000\n",
      "Epoch 597/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6797 - accuracy: 0.5125 - val_loss: 0.6771 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00597: val_accuracy did not improve from 0.83000\n",
      "Epoch 598/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6794 - accuracy: 0.5100 - val_loss: 0.6770 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00598: val_accuracy did not improve from 0.83000\n",
      "Epoch 599/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6790 - accuracy: 0.5238 - val_loss: 0.6764 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00599: val_accuracy did not improve from 0.83000\n",
      "Epoch 600/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6783 - accuracy: 0.5038 - val_loss: 0.6764 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00600: val_accuracy did not improve from 0.83000\n",
      "Epoch 601/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6774 - accuracy: 0.5387 - val_loss: 0.6764 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00601: val_accuracy did not improve from 0.83000\n",
      "Epoch 602/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6791 - accuracy: 0.5150 - val_loss: 0.6770 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00602: val_accuracy did not improve from 0.83000\n",
      "Epoch 603/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6772 - accuracy: 0.5238 - val_loss: 0.6766 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00603: val_accuracy did not improve from 0.83000\n",
      "Epoch 604/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6784 - accuracy: 0.5000 - val_loss: 0.6761 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00604: val_accuracy did not improve from 0.83000\n",
      "Epoch 605/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6773 - accuracy: 0.5163 - val_loss: 0.6762 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00605: val_accuracy did not improve from 0.83000\n",
      "Epoch 606/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6790 - accuracy: 0.5275 - val_loss: 0.6772 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00606: val_accuracy did not improve from 0.83000\n",
      "Epoch 607/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6794 - accuracy: 0.5000 - val_loss: 0.6769 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00607: val_accuracy did not improve from 0.83000\n",
      "Epoch 608/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6782 - accuracy: 0.5125 - val_loss: 0.6767 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00608: val_accuracy did not improve from 0.83000\n",
      "Epoch 609/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6793 - accuracy: 0.4975 - val_loss: 0.6772 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00609: val_accuracy did not improve from 0.83000\n",
      "Epoch 610/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6797 - accuracy: 0.4975 - val_loss: 0.6768 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00610: val_accuracy did not improve from 0.83000\n",
      "Epoch 611/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6792 - accuracy: 0.5200 - val_loss: 0.6765 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00611: val_accuracy did not improve from 0.83000\n",
      "Epoch 612/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6790 - accuracy: 0.4988 - val_loss: 0.6769 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00612: val_accuracy did not improve from 0.83000\n",
      "Epoch 613/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6781 - accuracy: 0.5038 - val_loss: 0.6767 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00613: val_accuracy did not improve from 0.83000\n",
      "Epoch 614/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6781 - accuracy: 0.5225 - val_loss: 0.6778 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00614: val_accuracy did not improve from 0.83000\n",
      "Epoch 615/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6791 - accuracy: 0.5150 - val_loss: 0.6773 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00615: val_accuracy did not improve from 0.83000\n",
      "Epoch 616/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6789 - accuracy: 0.5163 - val_loss: 0.6767 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00616: val_accuracy did not improve from 0.83000\n",
      "Epoch 617/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6785 - accuracy: 0.5350 - val_loss: 0.6767 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00617: val_accuracy did not improve from 0.83000\n",
      "Epoch 618/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6785 - accuracy: 0.4913 - val_loss: 0.6773 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00618: val_accuracy did not improve from 0.83000\n",
      "Epoch 619/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6802 - accuracy: 0.5075 - val_loss: 0.6759 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00619: val_accuracy did not improve from 0.83000\n",
      "Epoch 620/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6783 - accuracy: 0.5300 - val_loss: 0.6752 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00620: val_accuracy did not improve from 0.83000\n",
      "Epoch 621/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6785 - accuracy: 0.5125 - val_loss: 0.6760 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00621: val_accuracy did not improve from 0.83000\n",
      "Epoch 622/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6788 - accuracy: 0.5188 - val_loss: 0.6760 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00622: val_accuracy did not improve from 0.83000\n",
      "Epoch 623/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6790 - accuracy: 0.4888 - val_loss: 0.6764 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00623: val_accuracy did not improve from 0.83000\n",
      "Epoch 624/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6777 - accuracy: 0.5000 - val_loss: 0.6764 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00624: val_accuracy did not improve from 0.83000\n",
      "Epoch 625/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6778 - accuracy: 0.5238 - val_loss: 0.6765 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00625: val_accuracy did not improve from 0.83000\n",
      "Epoch 626/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6774 - accuracy: 0.5188 - val_loss: 0.6756 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00626: val_accuracy did not improve from 0.83000\n",
      "Epoch 627/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6783 - accuracy: 0.5025 - val_loss: 0.6759 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00627: val_accuracy did not improve from 0.83000\n",
      "Epoch 628/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6785 - accuracy: 0.5238 - val_loss: 0.6762 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00628: val_accuracy did not improve from 0.83000\n",
      "Epoch 629/2000\n",
      "800/800 [==============================] - 8s 9ms/step - loss: 0.6787 - accuracy: 0.4925 - val_loss: 0.6772 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00629: val_accuracy did not improve from 0.83000\n",
      "Epoch 630/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6783 - accuracy: 0.5000 - val_loss: 0.6773 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00630: val_accuracy did not improve from 0.83000\n",
      "Epoch 631/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6788 - accuracy: 0.5175 - val_loss: 0.6771 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00631: val_accuracy did not improve from 0.83000\n",
      "Epoch 632/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6792 - accuracy: 0.5013 - val_loss: 0.6766 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00632: val_accuracy did not improve from 0.83000\n",
      "Epoch 633/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6800 - accuracy: 0.4938 - val_loss: 0.6758 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00633: val_accuracy did not improve from 0.83000\n",
      "Epoch 634/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6769 - accuracy: 0.5138 - val_loss: 0.6755 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00634: val_accuracy did not improve from 0.83000\n",
      "Epoch 635/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6784 - accuracy: 0.5487 - val_loss: 0.6767 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00635: val_accuracy did not improve from 0.83000\n",
      "Epoch 636/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6788 - accuracy: 0.5288 - val_loss: 0.6762 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00636: val_accuracy did not improve from 0.83000\n",
      "Epoch 637/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6785 - accuracy: 0.5200 - val_loss: 0.6765 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00637: val_accuracy did not improve from 0.83000\n",
      "Epoch 638/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6792 - accuracy: 0.4975 - val_loss: 0.6769 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00638: val_accuracy did not improve from 0.83000\n",
      "Epoch 639/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6779 - accuracy: 0.5362 - val_loss: 0.6767 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00639: val_accuracy did not improve from 0.83000\n",
      "Epoch 640/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6792 - accuracy: 0.4925 - val_loss: 0.6768 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00640: val_accuracy did not improve from 0.83000\n",
      "Epoch 641/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6795 - accuracy: 0.5275 - val_loss: 0.6779 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00641: val_accuracy did not improve from 0.83000\n",
      "Epoch 642/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6794 - accuracy: 0.5088 - val_loss: 0.6777 - val_accuracy: 0.4550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00642: val_accuracy did not improve from 0.83000\n",
      "Epoch 643/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6788 - accuracy: 0.5163 - val_loss: 0.6764 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00643: val_accuracy did not improve from 0.83000\n",
      "Epoch 644/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6791 - accuracy: 0.4888 - val_loss: 0.6768 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00644: val_accuracy did not improve from 0.83000\n",
      "Epoch 645/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6800 - accuracy: 0.4863 - val_loss: 0.6764 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00645: val_accuracy did not improve from 0.83000\n",
      "Epoch 646/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6781 - accuracy: 0.4963 - val_loss: 0.6769 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00646: val_accuracy did not improve from 0.83000\n",
      "Epoch 647/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6779 - accuracy: 0.5412 - val_loss: 0.6767 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00647: val_accuracy did not improve from 0.83000\n",
      "Epoch 648/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6776 - accuracy: 0.5300 - val_loss: 0.6765 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00648: val_accuracy did not improve from 0.83000\n",
      "Epoch 649/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6786 - accuracy: 0.4900 - val_loss: 0.6767 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00649: val_accuracy did not improve from 0.83000\n",
      "Epoch 650/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6776 - accuracy: 0.4975 - val_loss: 0.6765 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00650: val_accuracy did not improve from 0.83000\n",
      "Epoch 651/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6790 - accuracy: 0.5013 - val_loss: 0.6765 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00651: val_accuracy did not improve from 0.83000\n",
      "Epoch 652/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6791 - accuracy: 0.5088 - val_loss: 0.6764 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00652: val_accuracy did not improve from 0.83000\n",
      "Epoch 653/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6784 - accuracy: 0.5038 - val_loss: 0.6766 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00653: val_accuracy did not improve from 0.83000\n",
      "Epoch 654/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6782 - accuracy: 0.5100 - val_loss: 0.6765 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00654: val_accuracy did not improve from 0.83000\n",
      "Epoch 655/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6779 - accuracy: 0.5400 - val_loss: 0.6759 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00655: val_accuracy did not improve from 0.83000\n",
      "Epoch 656/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6800 - accuracy: 0.5150 - val_loss: 0.6761 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00656: val_accuracy did not improve from 0.83000\n",
      "Epoch 657/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6792 - accuracy: 0.5188 - val_loss: 0.6762 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00657: val_accuracy did not improve from 0.83000\n",
      "Epoch 658/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6791 - accuracy: 0.4888 - val_loss: 0.6769 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00658: val_accuracy did not improve from 0.83000\n",
      "Epoch 659/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6799 - accuracy: 0.5088 - val_loss: 0.6767 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00659: val_accuracy did not improve from 0.83000\n",
      "Epoch 660/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6786 - accuracy: 0.5050 - val_loss: 0.6772 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00660: val_accuracy did not improve from 0.83000\n",
      "Epoch 661/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6791 - accuracy: 0.5238 - val_loss: 0.6769 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00661: val_accuracy did not improve from 0.83000\n",
      "Epoch 662/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6768 - accuracy: 0.5375 - val_loss: 0.6763 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00662: val_accuracy did not improve from 0.83000\n",
      "Epoch 663/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6783 - accuracy: 0.5225 - val_loss: 0.6758 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00663: val_accuracy did not improve from 0.83000\n",
      "Epoch 664/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6789 - accuracy: 0.5225 - val_loss: 0.6764 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00664: val_accuracy did not improve from 0.83000\n",
      "Epoch 665/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6792 - accuracy: 0.5013 - val_loss: 0.6768 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00665: val_accuracy did not improve from 0.83000\n",
      "Epoch 666/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6792 - accuracy: 0.4900 - val_loss: 0.6769 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00666: val_accuracy did not improve from 0.83000\n",
      "Epoch 667/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6807 - accuracy: 0.4988 - val_loss: 0.6774 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00667: val_accuracy did not improve from 0.83000\n",
      "Epoch 668/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6779 - accuracy: 0.5263 - val_loss: 0.6774 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00668: val_accuracy did not improve from 0.83000\n",
      "Epoch 669/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6787 - accuracy: 0.5238 - val_loss: 0.6773 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00669: val_accuracy did not improve from 0.83000\n",
      "Epoch 670/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6788 - accuracy: 0.5025 - val_loss: 0.6770 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00670: val_accuracy did not improve from 0.83000\n",
      "Epoch 671/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6787 - accuracy: 0.5088 - val_loss: 0.6764 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00671: val_accuracy did not improve from 0.83000\n",
      "Epoch 672/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6789 - accuracy: 0.5150 - val_loss: 0.6771 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00672: val_accuracy did not improve from 0.83000\n",
      "Epoch 673/2000\n",
      "800/800 [==============================] - 8s 9ms/step - loss: 0.6783 - accuracy: 0.4725 - val_loss: 0.6769 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00673: val_accuracy did not improve from 0.83000\n",
      "Epoch 674/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6793 - accuracy: 0.5100 - val_loss: 0.6761 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00674: val_accuracy did not improve from 0.83000\n",
      "Epoch 675/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6782 - accuracy: 0.5437 - val_loss: 0.6768 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00675: val_accuracy did not improve from 0.83000\n",
      "Epoch 676/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6788 - accuracy: 0.5412 - val_loss: 0.6762 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00676: val_accuracy did not improve from 0.83000\n",
      "Epoch 677/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6790 - accuracy: 0.5138 - val_loss: 0.6755 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00677: val_accuracy did not improve from 0.83000\n",
      "Epoch 678/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6789 - accuracy: 0.5138 - val_loss: 0.6764 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00678: val_accuracy did not improve from 0.83000\n",
      "Epoch 679/2000\n",
      "800/800 [==============================] - 8s 9ms/step - loss: 0.6800 - accuracy: 0.5150 - val_loss: 0.6762 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00679: val_accuracy did not improve from 0.83000\n",
      "Epoch 680/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6793 - accuracy: 0.5213 - val_loss: 0.6759 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00680: val_accuracy did not improve from 0.83000\n",
      "Epoch 681/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6787 - accuracy: 0.4988 - val_loss: 0.6764 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00681: val_accuracy did not improve from 0.83000\n",
      "Epoch 682/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6787 - accuracy: 0.5263 - val_loss: 0.6797 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00682: val_accuracy did not improve from 0.83000\n",
      "Epoch 683/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6790 - accuracy: 0.5063 - val_loss: 0.6796 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00683: val_accuracy did not improve from 0.83000\n",
      "Epoch 684/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6786 - accuracy: 0.4913 - val_loss: 0.6797 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00684: val_accuracy did not improve from 0.83000\n",
      "Epoch 685/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6786 - accuracy: 0.5487 - val_loss: 0.6797 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00685: val_accuracy did not improve from 0.83000\n",
      "Epoch 686/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6778 - accuracy: 0.5437 - val_loss: 0.6795 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00686: val_accuracy did not improve from 0.83000\n",
      "Epoch 687/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6781 - accuracy: 0.5175 - val_loss: 0.6797 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00687: val_accuracy did not improve from 0.83000\n",
      "Epoch 688/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6792 - accuracy: 0.5125 - val_loss: 0.6799 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00688: val_accuracy did not improve from 0.83000\n",
      "Epoch 689/2000\n",
      "800/800 [==============================] - 8s 9ms/step - loss: 0.6782 - accuracy: 0.5088 - val_loss: 0.6798 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00689: val_accuracy did not improve from 0.83000\n",
      "Epoch 690/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6786 - accuracy: 0.5063 - val_loss: 0.6801 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00690: val_accuracy did not improve from 0.83000\n",
      "Epoch 691/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6795 - accuracy: 0.5075 - val_loss: 0.6797 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00691: val_accuracy did not improve from 0.83000\n",
      "Epoch 692/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6775 - accuracy: 0.5275 - val_loss: 0.6798 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00692: val_accuracy did not improve from 0.83000\n",
      "Epoch 693/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6809 - accuracy: 0.5075 - val_loss: 0.6805 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00693: val_accuracy did not improve from 0.83000\n",
      "Epoch 694/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6776 - accuracy: 0.5325 - val_loss: 0.6799 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00694: val_accuracy did not improve from 0.83000\n",
      "Epoch 695/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6790 - accuracy: 0.5013 - val_loss: 0.6799 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00695: val_accuracy did not improve from 0.83000\n",
      "Epoch 696/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6804 - accuracy: 0.5050 - val_loss: 0.6800 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00696: val_accuracy did not improve from 0.83000\n",
      "Epoch 697/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6793 - accuracy: 0.5225 - val_loss: 0.6799 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00697: val_accuracy did not improve from 0.83000\n",
      "Epoch 698/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6780 - accuracy: 0.5013 - val_loss: 0.6795 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00698: val_accuracy did not improve from 0.83000\n",
      "Epoch 699/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6786 - accuracy: 0.5088 - val_loss: 0.6800 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00699: val_accuracy did not improve from 0.83000\n",
      "Epoch 700/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6785 - accuracy: 0.5188 - val_loss: 0.6800 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00700: val_accuracy did not improve from 0.83000\n",
      "Epoch 701/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6786 - accuracy: 0.5100 - val_loss: 0.6800 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00701: val_accuracy did not improve from 0.83000\n",
      "Epoch 702/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6789 - accuracy: 0.5238 - val_loss: 0.6801 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00702: val_accuracy did not improve from 0.83000\n",
      "Epoch 703/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6778 - accuracy: 0.5000 - val_loss: 0.6797 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00703: val_accuracy did not improve from 0.83000\n",
      "Epoch 704/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6783 - accuracy: 0.5325 - val_loss: 0.6799 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00704: val_accuracy did not improve from 0.83000\n",
      "Epoch 705/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6779 - accuracy: 0.5038 - val_loss: 0.6796 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00705: val_accuracy did not improve from 0.83000\n",
      "Epoch 706/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6792 - accuracy: 0.5038 - val_loss: 0.6799 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00706: val_accuracy did not improve from 0.83000\n",
      "Epoch 707/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6790 - accuracy: 0.4850 - val_loss: 0.6799 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00707: val_accuracy did not improve from 0.83000\n",
      "Epoch 708/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6785 - accuracy: 0.5063 - val_loss: 0.6799 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00708: val_accuracy did not improve from 0.83000\n",
      "Epoch 709/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6782 - accuracy: 0.5125 - val_loss: 0.6804 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00709: val_accuracy did not improve from 0.83000\n",
      "Epoch 710/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6779 - accuracy: 0.4938 - val_loss: 0.6799 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00710: val_accuracy did not improve from 0.83000\n",
      "Epoch 711/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6799 - accuracy: 0.5362 - val_loss: 0.6801 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00711: val_accuracy did not improve from 0.83000\n",
      "Epoch 712/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6789 - accuracy: 0.5038 - val_loss: 0.6804 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00712: val_accuracy did not improve from 0.83000\n",
      "Epoch 713/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6783 - accuracy: 0.5013 - val_loss: 0.6806 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00713: val_accuracy did not improve from 0.83000\n",
      "Epoch 714/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6782 - accuracy: 0.5275 - val_loss: 0.6804 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00714: val_accuracy did not improve from 0.83000\n",
      "Epoch 715/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6790 - accuracy: 0.4837 - val_loss: 0.6807 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00715: val_accuracy did not improve from 0.83000\n",
      "Epoch 716/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6793 - accuracy: 0.5175 - val_loss: 0.6804 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00716: val_accuracy did not improve from 0.83000\n",
      "Epoch 717/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6792 - accuracy: 0.5200 - val_loss: 0.6797 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00717: val_accuracy did not improve from 0.83000\n",
      "Epoch 718/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6783 - accuracy: 0.5075 - val_loss: 0.6800 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00718: val_accuracy did not improve from 0.83000\n",
      "Epoch 719/2000\n",
      "800/800 [==============================] - 10s 12ms/step - loss: 0.6778 - accuracy: 0.5275 - val_loss: 0.6801 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00719: val_accuracy did not improve from 0.83000\n",
      "Epoch 720/2000\n",
      "800/800 [==============================] - 10s 12ms/step - loss: 0.6787 - accuracy: 0.5437 - val_loss: 0.6801 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00720: val_accuracy did not improve from 0.83000\n",
      "Epoch 721/2000\n",
      "800/800 [==============================] - 10s 12ms/step - loss: 0.6782 - accuracy: 0.4812 - val_loss: 0.6799 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00721: val_accuracy did not improve from 0.83000\n",
      "Epoch 722/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6800 - accuracy: 0.5000 - val_loss: 0.6803 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00722: val_accuracy did not improve from 0.83000\n",
      "Epoch 723/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6790 - accuracy: 0.5100 - val_loss: 0.6801 - val_accuracy: 0.5650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00723: val_accuracy did not improve from 0.83000\n",
      "Epoch 724/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6792 - accuracy: 0.5200 - val_loss: 0.6805 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00724: val_accuracy did not improve from 0.83000\n",
      "Epoch 725/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6786 - accuracy: 0.5088 - val_loss: 0.6806 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00725: val_accuracy did not improve from 0.83000\n",
      "Epoch 726/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6788 - accuracy: 0.5138 - val_loss: 0.6804 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00726: val_accuracy did not improve from 0.83000\n",
      "Epoch 727/2000\n",
      "800/800 [==============================] - 9s 12ms/step - loss: 0.6798 - accuracy: 0.5063 - val_loss: 0.6801 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00727: val_accuracy did not improve from 0.83000\n",
      "Epoch 728/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6779 - accuracy: 0.5325 - val_loss: 0.6804 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00728: val_accuracy did not improve from 0.83000\n",
      "Epoch 729/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6778 - accuracy: 0.5200 - val_loss: 0.6803 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00729: val_accuracy did not improve from 0.83000\n",
      "Epoch 730/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6771 - accuracy: 0.5288 - val_loss: 0.6811 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00730: val_accuracy did not improve from 0.83000\n",
      "Epoch 731/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6782 - accuracy: 0.5275 - val_loss: 0.6804 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00731: val_accuracy did not improve from 0.83000\n",
      "Epoch 732/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6782 - accuracy: 0.5025 - val_loss: 0.6800 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00732: val_accuracy did not improve from 0.83000\n",
      "Epoch 733/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6790 - accuracy: 0.5100 - val_loss: 0.6795 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00733: val_accuracy did not improve from 0.83000\n",
      "Epoch 734/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6793 - accuracy: 0.5025 - val_loss: 0.6793 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00734: val_accuracy did not improve from 0.83000\n",
      "Epoch 735/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6785 - accuracy: 0.5050 - val_loss: 0.6796 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00735: val_accuracy did not improve from 0.83000\n",
      "Epoch 736/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6770 - accuracy: 0.5275 - val_loss: 0.6799 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00736: val_accuracy did not improve from 0.83000\n",
      "Epoch 737/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6805 - accuracy: 0.4625 - val_loss: 0.6797 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00737: val_accuracy did not improve from 0.83000\n",
      "Epoch 738/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6781 - accuracy: 0.5138 - val_loss: 0.6800 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00738: val_accuracy did not improve from 0.83000\n",
      "Epoch 739/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6787 - accuracy: 0.5350 - val_loss: 0.6803 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00739: val_accuracy did not improve from 0.83000\n",
      "Epoch 740/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6795 - accuracy: 0.5100 - val_loss: 0.6801 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00740: val_accuracy did not improve from 0.83000\n",
      "Epoch 741/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6786 - accuracy: 0.5300 - val_loss: 0.6795 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00741: val_accuracy did not improve from 0.83000\n",
      "Epoch 742/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6788 - accuracy: 0.5300 - val_loss: 0.6794 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00742: val_accuracy did not improve from 0.83000\n",
      "Epoch 743/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6783 - accuracy: 0.5175 - val_loss: 0.6797 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00743: val_accuracy did not improve from 0.83000\n",
      "Epoch 744/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6787 - accuracy: 0.5225 - val_loss: 0.6799 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00744: val_accuracy did not improve from 0.83000\n",
      "Epoch 745/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6782 - accuracy: 0.5350 - val_loss: 0.6794 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00745: val_accuracy did not improve from 0.83000\n",
      "Epoch 746/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6791 - accuracy: 0.4900 - val_loss: 0.6795 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00746: val_accuracy did not improve from 0.83000\n",
      "Epoch 747/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6778 - accuracy: 0.5100 - val_loss: 0.6797 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00747: val_accuracy did not improve from 0.83000\n",
      "Epoch 748/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6789 - accuracy: 0.4988 - val_loss: 0.6801 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00748: val_accuracy did not improve from 0.83000\n",
      "Epoch 749/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6786 - accuracy: 0.4988 - val_loss: 0.6792 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00749: val_accuracy did not improve from 0.83000\n",
      "Epoch 750/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6798 - accuracy: 0.5088 - val_loss: 0.6797 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00750: val_accuracy did not improve from 0.83000\n",
      "Epoch 751/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6774 - accuracy: 0.5150 - val_loss: 0.6802 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00751: val_accuracy did not improve from 0.83000\n",
      "Epoch 752/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6780 - accuracy: 0.5175 - val_loss: 0.6798 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00752: val_accuracy did not improve from 0.83000\n",
      "Epoch 753/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6794 - accuracy: 0.5175 - val_loss: 0.6806 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00753: val_accuracy did not improve from 0.83000\n",
      "Epoch 754/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6788 - accuracy: 0.4850 - val_loss: 0.6806 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00754: val_accuracy did not improve from 0.83000\n",
      "Epoch 755/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6789 - accuracy: 0.5250 - val_loss: 0.6803 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00755: val_accuracy did not improve from 0.83000\n",
      "Epoch 756/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6787 - accuracy: 0.5038 - val_loss: 0.6797 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00756: val_accuracy did not improve from 0.83000\n",
      "Epoch 757/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6809 - accuracy: 0.4938 - val_loss: 0.6794 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00757: val_accuracy did not improve from 0.83000\n",
      "Epoch 758/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6790 - accuracy: 0.4775 - val_loss: 0.6798 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00758: val_accuracy did not improve from 0.83000\n",
      "Epoch 759/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6794 - accuracy: 0.4787 - val_loss: 0.6791 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00759: val_accuracy did not improve from 0.83000\n",
      "Epoch 760/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6774 - accuracy: 0.5450 - val_loss: 0.6791 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00760: val_accuracy did not improve from 0.83000\n",
      "Epoch 761/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6793 - accuracy: 0.5113 - val_loss: 0.6794 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00761: val_accuracy did not improve from 0.83000\n",
      "Epoch 762/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6789 - accuracy: 0.5113 - val_loss: 0.6794 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00762: val_accuracy did not improve from 0.83000\n",
      "Epoch 763/2000\n",
      "800/800 [==============================] - 8s 9ms/step - loss: 0.6785 - accuracy: 0.5525 - val_loss: 0.6795 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00763: val_accuracy did not improve from 0.83000\n",
      "Epoch 764/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 8s 9ms/step - loss: 0.6788 - accuracy: 0.5225 - val_loss: 0.6797 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00764: val_accuracy did not improve from 0.83000\n",
      "Epoch 765/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6798 - accuracy: 0.4875 - val_loss: 0.6796 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00765: val_accuracy did not improve from 0.83000\n",
      "Epoch 766/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6789 - accuracy: 0.4913 - val_loss: 0.6801 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00766: val_accuracy did not improve from 0.83000\n",
      "Epoch 767/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6792 - accuracy: 0.4850 - val_loss: 0.6794 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00767: val_accuracy did not improve from 0.83000\n",
      "Epoch 768/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6782 - accuracy: 0.4950 - val_loss: 0.6794 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00768: val_accuracy did not improve from 0.83000\n",
      "Epoch 769/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6795 - accuracy: 0.5238 - val_loss: 0.6795 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00769: val_accuracy did not improve from 0.83000\n",
      "Epoch 770/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6786 - accuracy: 0.5050 - val_loss: 0.6796 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00770: val_accuracy did not improve from 0.83000\n",
      "Epoch 771/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6791 - accuracy: 0.4938 - val_loss: 0.6794 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00771: val_accuracy did not improve from 0.83000\n",
      "Epoch 772/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6790 - accuracy: 0.5462 - val_loss: 0.6790 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00772: val_accuracy did not improve from 0.83000\n",
      "Epoch 773/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6786 - accuracy: 0.4925 - val_loss: 0.6792 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00773: val_accuracy did not improve from 0.83000\n",
      "Epoch 774/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6776 - accuracy: 0.5088 - val_loss: 0.6796 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00774: val_accuracy did not improve from 0.83000\n",
      "Epoch 775/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6794 - accuracy: 0.5075 - val_loss: 0.6795 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00775: val_accuracy did not improve from 0.83000\n",
      "Epoch 776/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6788 - accuracy: 0.5088 - val_loss: 0.6799 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00776: val_accuracy did not improve from 0.83000\n",
      "Epoch 777/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6781 - accuracy: 0.5462 - val_loss: 0.6801 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00777: val_accuracy did not improve from 0.83000\n",
      "Epoch 778/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6777 - accuracy: 0.5075 - val_loss: 0.6802 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00778: val_accuracy did not improve from 0.83000\n",
      "Epoch 779/2000\n",
      "800/800 [==============================] - 8s 9ms/step - loss: 0.6781 - accuracy: 0.5200 - val_loss: 0.6798 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00779: val_accuracy did not improve from 0.83000\n",
      "Epoch 780/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6781 - accuracy: 0.4950 - val_loss: 0.6793 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00780: val_accuracy did not improve from 0.83000\n",
      "Epoch 781/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6774 - accuracy: 0.5013 - val_loss: 0.6789 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00781: val_accuracy did not improve from 0.83000\n",
      "Epoch 782/2000\n",
      "800/800 [==============================] - 8s 9ms/step - loss: 0.6776 - accuracy: 0.5088 - val_loss: 0.6789 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00782: val_accuracy did not improve from 0.83000\n",
      "Epoch 783/2000\n",
      "800/800 [==============================] - 8s 9ms/step - loss: 0.6777 - accuracy: 0.5275 - val_loss: 0.6787 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00783: val_accuracy did not improve from 0.83000\n",
      "Epoch 784/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6786 - accuracy: 0.5163 - val_loss: 0.6790 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00784: val_accuracy did not improve from 0.83000\n",
      "Epoch 785/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6793 - accuracy: 0.5250 - val_loss: 0.6794 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00785: val_accuracy did not improve from 0.83000\n",
      "Epoch 786/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6786 - accuracy: 0.4863 - val_loss: 0.6800 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00786: val_accuracy did not improve from 0.83000\n",
      "Epoch 787/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6784 - accuracy: 0.4825 - val_loss: 0.6802 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00787: val_accuracy did not improve from 0.83000\n",
      "Epoch 788/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6795 - accuracy: 0.4963 - val_loss: 0.6796 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00788: val_accuracy did not improve from 0.83000\n",
      "Epoch 789/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6785 - accuracy: 0.5150 - val_loss: 0.6799 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00789: val_accuracy did not improve from 0.83000\n",
      "Epoch 790/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6796 - accuracy: 0.5213 - val_loss: 0.6798 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00790: val_accuracy did not improve from 0.83000\n",
      "Epoch 791/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6779 - accuracy: 0.5075 - val_loss: 0.6799 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00791: val_accuracy did not improve from 0.83000\n",
      "Epoch 792/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6790 - accuracy: 0.5200 - val_loss: 0.6798 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00792: val_accuracy did not improve from 0.83000\n",
      "Epoch 793/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6789 - accuracy: 0.5400 - val_loss: 0.6799 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00793: val_accuracy did not improve from 0.83000\n",
      "Epoch 794/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6792 - accuracy: 0.4913 - val_loss: 0.6799 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00794: val_accuracy did not improve from 0.83000\n",
      "Epoch 795/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6779 - accuracy: 0.5088 - val_loss: 0.6799 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00795: val_accuracy did not improve from 0.83000\n",
      "Epoch 796/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6785 - accuracy: 0.4988 - val_loss: 0.6796 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00796: val_accuracy did not improve from 0.83000\n",
      "Epoch 797/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6788 - accuracy: 0.5362 - val_loss: 0.6798 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00797: val_accuracy did not improve from 0.83000\n",
      "Epoch 798/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6782 - accuracy: 0.5213 - val_loss: 0.6805 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00798: val_accuracy did not improve from 0.83000\n",
      "Epoch 799/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6774 - accuracy: 0.5512 - val_loss: 0.6801 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00799: val_accuracy did not improve from 0.83000\n",
      "Epoch 800/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6786 - accuracy: 0.5450 - val_loss: 0.6799 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00800: val_accuracy did not improve from 0.83000\n",
      "Epoch 801/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6790 - accuracy: 0.5188 - val_loss: 0.6798 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00801: val_accuracy did not improve from 0.83000\n",
      "Epoch 802/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6781 - accuracy: 0.5288 - val_loss: 0.6793 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00802: val_accuracy did not improve from 0.83000\n",
      "Epoch 803/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6781 - accuracy: 0.5050 - val_loss: 0.6793 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00803: val_accuracy did not improve from 0.83000\n",
      "Epoch 804/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6776 - accuracy: 0.5263 - val_loss: 0.6790 - val_accuracy: 0.5650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00804: val_accuracy did not improve from 0.83000\n",
      "Epoch 805/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6787 - accuracy: 0.5325 - val_loss: 0.6794 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00805: val_accuracy did not improve from 0.83000\n",
      "Epoch 806/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6787 - accuracy: 0.5075 - val_loss: 0.6798 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00806: val_accuracy did not improve from 0.83000\n",
      "Epoch 807/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6773 - accuracy: 0.5088 - val_loss: 0.6803 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00807: val_accuracy did not improve from 0.83000\n",
      "Epoch 808/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6778 - accuracy: 0.5213 - val_loss: 0.6799 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00808: val_accuracy did not improve from 0.83000\n",
      "Epoch 809/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6794 - accuracy: 0.5088 - val_loss: 0.6801 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00809: val_accuracy did not improve from 0.83000\n",
      "Epoch 810/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6791 - accuracy: 0.4913 - val_loss: 0.6800 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00810: val_accuracy did not improve from 0.83000\n",
      "Epoch 811/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6781 - accuracy: 0.5125 - val_loss: 0.6804 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00811: val_accuracy did not improve from 0.83000\n",
      "Epoch 812/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6789 - accuracy: 0.5088 - val_loss: 0.6803 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00812: val_accuracy did not improve from 0.83000\n",
      "Epoch 813/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6779 - accuracy: 0.5450 - val_loss: 0.6798 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00813: val_accuracy did not improve from 0.83000\n",
      "Epoch 814/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6790 - accuracy: 0.5000 - val_loss: 0.6801 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00814: val_accuracy did not improve from 0.83000\n",
      "Epoch 815/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6788 - accuracy: 0.5038 - val_loss: 0.6797 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00815: val_accuracy did not improve from 0.83000\n",
      "Epoch 816/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6774 - accuracy: 0.5387 - val_loss: 0.6794 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00816: val_accuracy did not improve from 0.83000\n",
      "Epoch 817/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6785 - accuracy: 0.5213 - val_loss: 0.6789 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00817: val_accuracy did not improve from 0.83000\n",
      "Epoch 818/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6807 - accuracy: 0.5138 - val_loss: 0.6790 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00818: val_accuracy did not improve from 0.83000\n",
      "Epoch 819/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6786 - accuracy: 0.5225 - val_loss: 0.6797 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00819: val_accuracy did not improve from 0.83000\n",
      "Epoch 820/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6780 - accuracy: 0.5175 - val_loss: 0.6800 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00820: val_accuracy did not improve from 0.83000\n",
      "Epoch 821/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6775 - accuracy: 0.5250 - val_loss: 0.6798 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00821: val_accuracy did not improve from 0.83000\n",
      "Epoch 822/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6778 - accuracy: 0.5600 - val_loss: 0.6799 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00822: val_accuracy did not improve from 0.83000\n",
      "Epoch 823/2000\n",
      "800/800 [==============================] - 8s 11ms/step - loss: 0.6782 - accuracy: 0.5250 - val_loss: 0.6795 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00823: val_accuracy did not improve from 0.83000\n",
      "Epoch 824/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6791 - accuracy: 0.5063 - val_loss: 0.6792 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00824: val_accuracy did not improve from 0.83000\n",
      "Epoch 825/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6798 - accuracy: 0.5150 - val_loss: 0.6794 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00825: val_accuracy did not improve from 0.83000\n",
      "Epoch 826/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6791 - accuracy: 0.4963 - val_loss: 0.6793 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00826: val_accuracy did not improve from 0.83000\n",
      "Epoch 827/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6778 - accuracy: 0.5263 - val_loss: 0.6795 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00827: val_accuracy did not improve from 0.83000\n",
      "Epoch 828/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6790 - accuracy: 0.5163 - val_loss: 0.6799 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00828: val_accuracy did not improve from 0.83000\n",
      "Epoch 829/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6778 - accuracy: 0.5350 - val_loss: 0.6800 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00829: val_accuracy did not improve from 0.83000\n",
      "Epoch 830/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6773 - accuracy: 0.5188 - val_loss: 0.6798 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00830: val_accuracy did not improve from 0.83000\n",
      "Epoch 831/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6797 - accuracy: 0.4975 - val_loss: 0.6801 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00831: val_accuracy did not improve from 0.83000\n",
      "Epoch 832/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6799 - accuracy: 0.4675 - val_loss: 0.6807 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00832: val_accuracy did not improve from 0.83000\n",
      "Epoch 833/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6787 - accuracy: 0.5050 - val_loss: 0.6798 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00833: val_accuracy did not improve from 0.83000\n",
      "Epoch 834/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6792 - accuracy: 0.5175 - val_loss: 0.6798 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00834: val_accuracy did not improve from 0.83000\n",
      "Epoch 835/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6778 - accuracy: 0.5288 - val_loss: 0.6797 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00835: val_accuracy did not improve from 0.83000\n",
      "Epoch 836/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6799 - accuracy: 0.5375 - val_loss: 0.6793 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00836: val_accuracy did not improve from 0.83000\n",
      "Epoch 837/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6794 - accuracy: 0.5250 - val_loss: 0.6795 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00837: val_accuracy did not improve from 0.83000\n",
      "Epoch 838/2000\n",
      "800/800 [==============================] - 8s 9ms/step - loss: 0.6794 - accuracy: 0.5125 - val_loss: 0.6796 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00838: val_accuracy did not improve from 0.83000\n",
      "Epoch 839/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6793 - accuracy: 0.4988 - val_loss: 0.6791 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00839: val_accuracy did not improve from 0.83000\n",
      "Epoch 840/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6783 - accuracy: 0.5250 - val_loss: 0.6793 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00840: val_accuracy did not improve from 0.83000\n",
      "Epoch 841/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6775 - accuracy: 0.5188 - val_loss: 0.6798 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00841: val_accuracy did not improve from 0.83000\n",
      "Epoch 842/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6803 - accuracy: 0.4938 - val_loss: 0.6799 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00842: val_accuracy did not improve from 0.83000\n",
      "Epoch 843/2000\n",
      "800/800 [==============================] - 8s 9ms/step - loss: 0.6785 - accuracy: 0.5138 - val_loss: 0.6802 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00843: val_accuracy did not improve from 0.83000\n",
      "Epoch 844/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6797 - accuracy: 0.4950 - val_loss: 0.6802 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00844: val_accuracy did not improve from 0.83000\n",
      "Epoch 845/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 8s 11ms/step - loss: 0.6789 - accuracy: 0.5138 - val_loss: 0.6803 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00845: val_accuracy did not improve from 0.83000\n",
      "Epoch 846/2000\n",
      "800/800 [==============================] - 8s 9ms/step - loss: 0.6789 - accuracy: 0.5075 - val_loss: 0.6804 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00846: val_accuracy did not improve from 0.83000\n",
      "Epoch 847/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6792 - accuracy: 0.5088 - val_loss: 0.6803 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00847: val_accuracy did not improve from 0.83000\n",
      "Epoch 848/2000\n",
      "800/800 [==============================] - 8s 11ms/step - loss: 0.6776 - accuracy: 0.5275 - val_loss: 0.6803 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00848: val_accuracy did not improve from 0.83000\n",
      "Epoch 849/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6787 - accuracy: 0.5088 - val_loss: 0.6802 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00849: val_accuracy did not improve from 0.83000\n",
      "Epoch 850/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6785 - accuracy: 0.5013 - val_loss: 0.6801 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00850: val_accuracy did not improve from 0.83000\n",
      "Epoch 851/2000\n",
      "800/800 [==============================] - 8s 9ms/step - loss: 0.6781 - accuracy: 0.5113 - val_loss: 0.6798 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00851: val_accuracy did not improve from 0.83000\n",
      "Epoch 852/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6782 - accuracy: 0.5138 - val_loss: 0.6794 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00852: val_accuracy did not improve from 0.83000\n",
      "Epoch 853/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6783 - accuracy: 0.5063 - val_loss: 0.6800 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00853: val_accuracy did not improve from 0.83000\n",
      "Epoch 854/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6798 - accuracy: 0.5038 - val_loss: 0.6803 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00854: val_accuracy did not improve from 0.83000\n",
      "Epoch 855/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6783 - accuracy: 0.5188 - val_loss: 0.6801 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00855: val_accuracy did not improve from 0.83000\n",
      "Epoch 856/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6790 - accuracy: 0.5000 - val_loss: 0.6802 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00856: val_accuracy did not improve from 0.83000\n",
      "Epoch 857/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6792 - accuracy: 0.4863 - val_loss: 0.6801 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00857: val_accuracy did not improve from 0.83000\n",
      "Epoch 858/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6791 - accuracy: 0.5000 - val_loss: 0.6797 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00858: val_accuracy did not improve from 0.83000\n",
      "Epoch 859/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6759 - accuracy: 0.4787 - val_loss: 0.6796 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00859: val_accuracy did not improve from 0.83000\n",
      "Epoch 860/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6749 - accuracy: 0.5375 - val_loss: 0.6794 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00860: val_accuracy did not improve from 0.83000\n",
      "Epoch 861/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6754 - accuracy: 0.5075 - val_loss: 0.6796 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00861: val_accuracy did not improve from 0.83000\n",
      "Epoch 862/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6749 - accuracy: 0.5300 - val_loss: 0.6795 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00862: val_accuracy did not improve from 0.83000\n",
      "Epoch 863/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6752 - accuracy: 0.5000 - val_loss: 0.6788 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00863: val_accuracy did not improve from 0.83000\n",
      "Epoch 864/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6750 - accuracy: 0.5113 - val_loss: 0.6788 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00864: val_accuracy did not improve from 0.83000\n",
      "Epoch 865/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6751 - accuracy: 0.5088 - val_loss: 0.6785 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00865: val_accuracy did not improve from 0.83000\n",
      "Epoch 866/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6745 - accuracy: 0.5113 - val_loss: 0.6787 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00866: val_accuracy did not improve from 0.83000\n",
      "Epoch 867/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6747 - accuracy: 0.5125 - val_loss: 0.6790 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00867: val_accuracy did not improve from 0.83000\n",
      "Epoch 868/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6751 - accuracy: 0.4900 - val_loss: 0.6793 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00868: val_accuracy did not improve from 0.83000\n",
      "Epoch 869/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6747 - accuracy: 0.5175 - val_loss: 0.6789 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00869: val_accuracy did not improve from 0.83000\n",
      "Epoch 870/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6741 - accuracy: 0.5163 - val_loss: 0.6789 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00870: val_accuracy did not improve from 0.83000\n",
      "Epoch 871/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6744 - accuracy: 0.5437 - val_loss: 0.6788 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00871: val_accuracy did not improve from 0.83000\n",
      "Epoch 872/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6750 - accuracy: 0.5238 - val_loss: 0.6788 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00872: val_accuracy did not improve from 0.83000\n",
      "Epoch 873/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6748 - accuracy: 0.5025 - val_loss: 0.6790 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00873: val_accuracy did not improve from 0.83000\n",
      "Epoch 874/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6744 - accuracy: 0.5238 - val_loss: 0.6790 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00874: val_accuracy did not improve from 0.83000\n",
      "Epoch 875/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6746 - accuracy: 0.5088 - val_loss: 0.6792 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00875: val_accuracy did not improve from 0.83000\n",
      "Epoch 876/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6743 - accuracy: 0.5213 - val_loss: 0.6790 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00876: val_accuracy did not improve from 0.83000\n",
      "Epoch 877/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6750 - accuracy: 0.4888 - val_loss: 0.6794 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00877: val_accuracy did not improve from 0.83000\n",
      "Epoch 878/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6744 - accuracy: 0.5225 - val_loss: 0.6799 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00878: val_accuracy did not improve from 0.83000\n",
      "Epoch 879/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6739 - accuracy: 0.5075 - val_loss: 0.6801 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00879: val_accuracy did not improve from 0.83000\n",
      "Epoch 880/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6740 - accuracy: 0.5362 - val_loss: 0.6798 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00880: val_accuracy did not improve from 0.83000\n",
      "Epoch 881/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6742 - accuracy: 0.5263 - val_loss: 0.6793 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00881: val_accuracy did not improve from 0.83000\n",
      "Epoch 882/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6747 - accuracy: 0.4825 - val_loss: 0.6798 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00882: val_accuracy did not improve from 0.83000\n",
      "Epoch 883/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6748 - accuracy: 0.5100 - val_loss: 0.6790 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00883: val_accuracy did not improve from 0.83000\n",
      "Epoch 884/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6752 - accuracy: 0.4925 - val_loss: 0.6787 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00884: val_accuracy did not improve from 0.83000\n",
      "Epoch 885/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6745 - accuracy: 0.5088 - val_loss: 0.6788 - val_accuracy: 0.5650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00885: val_accuracy did not improve from 0.83000\n",
      "Epoch 886/2000\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.6746 - accuracy: 0.5088 - val_loss: 0.6787 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00886: val_accuracy did not improve from 0.83000\n",
      "Epoch 887/2000\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6738 - accuracy: 0.5312 - val_loss: 0.6781 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00887: val_accuracy did not improve from 0.83000\n",
      "Epoch 888/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6735 - accuracy: 0.5163 - val_loss: 0.6777 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00888: val_accuracy did not improve from 0.83000\n",
      "Epoch 889/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6735 - accuracy: 0.5213 - val_loss: 0.6770 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00889: val_accuracy did not improve from 0.83000\n",
      "Epoch 890/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6745 - accuracy: 0.5100 - val_loss: 0.6775 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00890: val_accuracy did not improve from 0.83000\n",
      "Epoch 891/2000\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.6741 - accuracy: 0.5175 - val_loss: 0.6780 - val_accuracy: 0.5650\n",
      "\n",
      "Epoch 00891: val_accuracy did not improve from 0.83000\n",
      "Epoch 892/2000\n",
      "768/800 [===========================>..] - ETA: 0s - loss: 0.6748 - accuracy: 0.5000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-1654b5d91180>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m           \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m           callbacks=callbacks(net_name))\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Visualize History of Traing model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 2000\n",
    "\n",
    "net_name = NET_NAME + str(epochs)\n",
    "model.fit(X_train, Y_train, \n",
    "          epochs=epochs, \n",
    "          batch_size=64, \n",
    "          workers=4,\n",
    "          shuffle=True, \n",
    "          validation_data=(X_test, Y_test), \n",
    "          callbacks=callbacks(net_name))\n",
    "\n",
    "# Visualize History of Traing model\n",
    "show_training_history(\"./model/\"+NET_NAME + str(epochs)+\"-history.txt\")\n",
    "# Evaluate model, loss and accuracy\n",
    "loss, acc = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print()\n",
    "print(\"Test accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from model/sentiment-1107-weights-31-0.10-0.98-0.69-0.84.hdf5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You are trying to load a weight file containing 6 layers into a model with 4 layers.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-a5af1f7ee0ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'model/sentiment-1107-weights-31-0.10-0.98-0.69-0.84.hdf5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading weights from\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Evaluate model, loss and accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mload_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, reshape)\u001b[0m\n\u001b[1;32m   1228\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1229\u001b[0m                 saving.load_weights_from_hdf5_group(\n\u001b[0;32m-> 1230\u001b[0;31m                     f, self.layers, reshape=reshape)\n\u001b[0m\u001b[1;32m   1231\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'close'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers, reshape)\u001b[0m\n\u001b[1;32m   1207\u001b[0m                          \u001b[0;34m'containing '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m                          \u001b[0;34m' layers into a model with '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1209\u001b[0;31m                          str(len(filtered_layers)) + ' layers.')\n\u001b[0m\u001b[1;32m   1210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1211\u001b[0m     \u001b[0;31m# We batch weight value assignments in a single backend call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: You are trying to load a weight file containing 6 layers into a model with 4 layers."
     ]
    }
   ],
   "source": [
    "model_path = 'model/sentiment-1107-weights-31-0.10-0.98-0.69-0.84.hdf5'\n",
    "print(\"Loading weights from\", model_path)\n",
    "model.load_weights(model_path)\n",
    "\n",
    "# Evaluate model, loss and accuracy\n",
    "loss, acc = model.evaluate(X, Y, verbose=1)\n",
    "print()\n",
    "print(\"Test accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt \n",
    "\n",
    "def show_training_history(file_path):\n",
    "    histories=np.loadtxt(file_path, delimiter=\",\")\n",
    "    plt.plot(histories[:, 1])\n",
    "    plt.plot(histories[:, 3])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    # summarize history for loss\n",
    "    plt.plot(histories[:, 0])\n",
    "    plt.plot(histories[:, 2])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "show_training_history(\"./model/sentiment-history.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
